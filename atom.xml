<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title><![CDATA[Tech.Semicolon]]></title>
    <link href="https://techsemicolon.github.io/atom.xml" rel="self"/>
    <link href="https://techsemicolon.github.io/"/>
    <updated>2019-10-10T12:02:56+00:00</updated>
    <id>https://techsemicolon.github.io/</id>
        <generator uri="http://sculpin.io/">Sculpin</generator>
            <entry>
            <title type="html"><![CDATA[PHP OPcache important settings and revalidation simplified]]></title>
            <link href="https://techsemicolon.github.io/blog/2019/10/05/php-fpm-and-opcache-settings-and-invalidation-simplified/"/>
            <updated>2019-10-05T00:00:00+00:00</updated>
            <id>https://techsemicolon.github.io/blog/2019/10/05/php-fpm-and-opcache-settings-and-invalidation-simplified/</id>
            <content type="html"><![CDATA[<h3 id="about-opcache-%3A">About OPcache :</h3>

<p>PHP as a language is an interpreted. But the interpretation happens from the binaries. These binaries are compiled into intermediate bytecodes. OPcache comes into picture during the compilation phase where it stores the precompiled script's bytecodes into a shared memory. When next time PHP needs to compile the same PHP script, the precompile bytecodes from the shared memory are used.. Therefore removing the requirement for PHP to load and parse scripts on each request. As a result improving overall PHP performance.</p>

<p>The OPcache extension is bundled with PHP 5.5.0 and later versions.</p>

<h3 id="why-it-is-needed-%3A">Why it is needed :</h3>

<p>Okay wait... Let's really get into a very vital need of this OPcache in the ever growing world of PHP frameworks. Any PHP framework has some core package files which we never change. These php package files really let us use the framework capabilities. For example for laravel, we have <code>illuminate</code>'s laraval packages. Now on top of that, we install new PHP packages as and when needed using dependency managers like <code>composer</code>. We hardly change these files unless we update/install/remove a package dependency.</p>

<p>Now, independent of which PHP framework you are using, these core PHP script files which constitute framework more or less are compiled for each request to generate their bytecodes for interpretation. Similarly, the dependency package PHP script are compied as per their need for that particular request.</p>

<p>Why do we really need to compile these core PHP script files which we indeed hardly change?? Isn't that an overly unneccessary overhead for PHP compilation process to compile these hundreds of PHP script files on each request and thenm intrepret them? Doesn't make much sense right? There you go.. OPcache to the rescue...</p>

<h3 id="important-settings-of-opcache-%3A">Important settings of OPcache :</h3>

<p>As OPcache overall sounds really fascinating which internally optimizes the compilation process of PHP scripts, it is equally important to know the important settings. Cache handling, cache clearing and cache storing and overall invalidation depends on these settings.</p>

<ul>
<li>Finding the php.ini to work with : 
You need to find the php.ini which is currently in use. There are different ways to find it. You can find it from terminal using :</li>
</ul>

<pre><code class="bash">php --ini | grep -i loaded
</code></pre>

<p>OR create a temporary script file called 'info.php` and have following content on it :</p>

<pre><code class="php">phpinfo();
</code></pre>

<p>Once you open this file into browser, you would be able to see which php.ini is used.</p>

<p>Mostly if you have php version 7.x with fpm installed then it will be inside <code>/etc/php/7.x/fpm/php.ini</code>. Or if you have 7.x with apache2 installed then it will be inside <code>/etc/php/7.x/apache2/php.ini</code>.</p>

<ul>
<li><p>Enable opcache :</p>

<p>In the loaded and used <code>php.ini</code>, you can enable and disable opcache by updating following directive :</p>

<pre><code>opcache.enable=1 //Enables OPcache
opcache.enable=0 //Disables OPcache
</code></pre>

<p>OPCache also can be enabled for CLI, PHP running from the command line interface(terminal) :</p>

<pre><code>opcache.enable_cli=1 //Enables OPcache
opcache.enable_cli=0 //Disables OPcache
</code></pre></li>
<li><p>OPcache RAM usage :</p>

<p>When the entire cache is stored into shared memory, the default max size of that memory storage is <code>64MB</code> before PHP <code>7.0.0</code> and <code>128MB</code> for after versions. You can update it as per your memory. Ideally 64MB to 128MB size is more than enough for most of the PHP frameworks.
In the loaded and used <code>php.ini</code>, you can change this setting by updating following directive :</p>

<pre><code>opcache.memory_consumption=128
</code></pre></li>
<li><p>Cached script limit :</p>

<p>By default OPcache can cache <code>2000</code> files before PHP <code>7.0.0</code> and <code>10000</code> for after versions. You can increase this number as per your requirement. For frameworks like PHP Magento you could probably have this value bumped up as the core files are large in number.</p>

<p>First know count of PHP scripting files in your codebase :</p>

<pre><code class="bash"># Make sure you change this path as per your setup
cd  /var/www/html 
# Count number of php files
find . -type f -print | grep php | wc -l
</code></pre>

<p>Now above will givbe you a numeric count of total PHP script files in your codebase. Now you can set OPcache setting accordingly. In the loaded and used <code>php.ini</code>, you can change this setting by updating following directive :</p>

<pre><code>opcache.max_accelerated_files=20000
</code></pre></li>
<li><p>Set automated cache clearing :</p>

<p>When you have new updates in cached PHP script files, OPcache will attempt to clear and then revalidate their cache. This is set using <code>validate_timestamps</code> directive and it happens periodically based on other directive setting <code>revalidate_freq</code>. When this directive is disabled, you MUST reset OPcache manually via opcache_reset(), opcache_invalidate() or by restarting the Web server for changes to the filesystem to take effect.
In the loaded and used <code>php.ini</code>, you can change this setting by updating following directive :</p>

<pre><code>opcache.validate_timestamps=1 //Enabled
opcache.validate_timestamps=0 //Disabled
</code></pre></li>
<li><p>Revalidation and it's frequency :</p>

<p>OPcache revalidates the opcache afer certain number of seconds to check if your code has changed. <code>0</code> value signifies that it checks your PHP code every single request which is unneccessary.
By default it happens every 2 seconds. If you have PHP files changing hardly on your setup, it makes sence to increase the frequency duration. As revalidation also takes memory, bumping this up might save unneccessary memory revalidation usage.
In the loaded and used <code>php.ini</code>, you can change this setting by updating following directive :</p>

<pre><code>opcache.revalidate_freq=120
</code></pre>

<p>OR you can totally disable this revalidation by updating following directive :</p>

<pre><code>opcache.validate_timestamps=0 //Disables OPcache revalidation
</code></pre>

<p>In your development or sandbox environment you can set this to 0 so that changes take effect immediately. However, it is as good as disabling which can save lot of syscalls.</p></li>
<li><p>Skipping caching selecively :</p>

<p>If you know that let's say <code>user.php</code> file in your framework changes often and you do not really want to clear cache everytime it changes. You can basically have OPcache ignore the caching of this file. It is called blacklisting file in terms of OPcache. This setting is littlbit different. It takes an absolute path location of a <code>.txt</code> file. In this text file you can specify the files you want to blacklist(skip) for caching.
In the loaded and used <code>php.ini</code>, you can change this setting by updating following directive :</p>

<pre><code>opcache.blacklist_filename=/etc/php/7.x/fpm/opcache_blacklist.txt
</code></pre>

<p>You can have whatever name you want for this text file. And then inside <code>opcache_blacklist.txt</code> :</p>

<pre><code class="bash">#Specific file inside /var/www/html/
/var/www/html/user.php 

#All files starting with user* inside /var/www/html/
/var/www/html/user

#Wildcare usage inside /var/www/html/
/var/www/*-user.php
</code></pre></li>
<li><p>Increasing request response shutdown :</p>

<p>PHP OPcache uses a mechanism which checks for repetitive occurances of strings and internally stores single value of it with remaining ones having pointer to the same value. The fascinating thing is instead of having a pool of these strings for each and every FPM process, this shares it across all of your FPM processes.</p>

<p>In OPcache settings, you can control how much memory this process can use from directive <code>opcache.interned_strings_buffer</code> Its default value is <code>4</code>(MB) before PHP <code>7.0.0</code> and <code>8</code>(MB) for after versions.</p>

<p>In the loaded and used <code>php.ini</code>, you can change this setting by updating following directive :</p>

<pre><code>opcache.interned_strings_buffer=12
</code></pre></li>
</ul>

<p>Note that to have changes to take effect from php.ini, you need to restart PHP FPM or apache2 depending on your setup.</p>

<h3 id="clearing-the-opcache-%3A">Clearing the OPcache :</h3>

<p>When OPcache is enabled, any changes in cached PHP script files will not take effect until OPcache is cleared or it is revalidated. This is applicable when you release new updates into a OPcache enabled PHP server.</p>

<p>To clear cache there are multiple ways :</p>

<ul>
<li><p>Clearing from browser :</p>

<p>You can have a file <code>opcache_refresh.php</code> with following content :</p>

<pre><code class="php">try{
    opcache_reset();
    echo "OPcache has been cleared successfully!";
}
catch(\Exception $e){
    echo "Oops.. OPcache could not be cleared!";
}
</code></pre>

<p>When you request this file from browser, it will reset(clear) the opcache and any new changes done inside the cached files will start reflecting.</p></li>
<li><p>Clearing from terminal :</p>

<p>If you have nginx with PHP FPM :</p>

<pre><code class="bash">sudo service php7.x-fpm reload
</code></pre>

<p>If you have apache2 :</p>

<pre><code class="bash">sudo service apache2 reload
</code></pre></li>
<li><p>Using cachetool :</p>

<p>This is very useful when you do not want to have an open PHP file to reset opcache and you do not want to reload FPM or apache2 for the same. <a href="http://gordalina.github.io/cachetool/"><code>cachetool</code></a> is a way between the two :</p>

<p>To Install  :</p>

<pre><code class="bash">// Download the phar
curl -sO http://gordalina.github.io/cachetool/downloads/cachetool.phar
// Set permissions
chmod +x cachetool.phar
</code></pre>

<p>Now you can clear opcache using following command :</p>

<pre><code class="bash">// Using an automatically guessed fastcgi server
php cachetool.phar opcache:reset --fcgi

// Using php fpm socket
php cachetool.phar opcache:reset --fcgi=/var/run/php7.x-fpm.sock
</code></pre></li>
</ul>

<h3 id="opcache-and-the-automated-deployments-%3A">OPcache and the automated deployments :</h3>

<p>When you have automated deployments set up on your PHP server, you need to incorporate OPcache clearing so that new changes take effect.</p>

<p>If you do not perform this manually, eventually OPcache will revalidate itself based on your OPcache revalidation settings assuming you have <code>validate_timestamps</code> set to <code>1</code>. But it is pretty handy to do it manually as part of your automated deployment scripts to make sure changes take immediate effect.</p>

<p>From my personal experience, relading FPM or apache2 to clear OPcache is not a good option. Even if it gracefully reloads PHP processes, any request which is(unfortunarely) ongoing on the server gives <code>502 bad gateway</code>. I would rather use the alternative like <code>cachetool</code> mentioned in above section which does not affect any ongoing PHP processes at all.</p>

<p>The sequence when you reload OPcache matters. As soon as git or any of your deployment methods pulls the latest PHP code changes, you should first clear the OPcache. Then you can run all the remaining deployment commands which may use the updated PHP files. This makes sure the deployment commands do not use cached PHP script opcodes even when the sever has pulled the latest changes.</p>
]]></content>
        </entry>
            <entry>
            <title type="html"><![CDATA[Manage laravel .env file using AWS parameter store]]></title>
            <link href="https://techsemicolon.github.io/blog/2019/07/10/aws-manage-env-file-using-ssm-laravel/"/>
            <updated>2019-07-10T00:00:00+00:00</updated>
            <id>https://techsemicolon.github.io/blog/2019/07/10/aws-manage-env-file-using-ssm-laravel/</id>
            <content type="html"><![CDATA[<p>When you have an application hosted on AWS EC2 instances which runs on an environment file, like laravel framework needs <code>.env</code> environment file, it is always a pain to manage environment variables.</p>

<p>Specially when you have multiple EC2 instances running for production on an auto-scaling setup. Please note that these are not host OS environment variable, but the application framework's environment variables in the respective env file.</p>

<p>You may alternatively call them application configuration files. You mostly do not add them in git or bitbucket due to the possibility of them containing sensitive information, e.g. database passwords, AWS access credentials etc..</p>

<h3 id="problem-statement-%3A">Problem statement :</h3>

<ul>
<li>You have environment .env file inside your EC2 AMI</li>
<li>When you spin up an instance, the .env file comes from the AMI specified in the launch configuration, considering you have auto-scaling set up</li>
<li>If you want to update any existing .env variable, you need to spin up new AMI and update production servers</li>
</ul>

<p>If your production environment has a setup like above, you may have experienced bit of pain in changing .env variables. Specially when there is a quick turnaround required due to some urgency or bug where you need to update an .env file variable asap.</p>

<h3 id="ec2-user_data-coming-to-rescue-%3A">EC2 user_data coming to rescue :</h3>

<p>When you spin up a new instance, you have an option in <code>Step 3: Configure Instance Details</code> screen inside <code>Advanced Details</code> called <code>user_data</code>. This looks something like below :</p>

<p>You can write shell scripts in text area or upload a bash file as per your choice. When you launch an instance in Amazon EC2, these shell scripts will be run after the instance starts. You can also pass cloud-init directives but we will stick to shell script for this use case to fetch the environment variables from systems manager parameter store and generate an environment file, in our case <code>.env</code> file under laravel root.</p>

<p><img src="/images/aws-ansible/user-data.png" alt="alt text1" /></p>

<h3 id="setting-up-variables-in-parameter-store-%3A">Setting up variables in parameter store :</h3>

<p>AWS has service called <code>Systems Manager</code> which contains a sub-service for resource sharing. This is called <code>Parameter Store</code>. There are couple of ways to store a parameter inside this service. We will be using <code>SecureString</code> option which makes sure the paramater value(which in our case is the enviromnent file contents) are encrypted inside AWS.</p>

<p>You may think that instead of storing entire .env file content into a single text-area, why should we not create multiple paremeters for each variable in the .env file? That's a perfectly valid point. However, storing it in one paramter store decreases overall maintenability and also makes the shell script to retrive those values very compact.</p>

<p>You can use below steps to store your <code>.env</code> file inside parameter store :</p>

<ol>
<li>Login to AWS console and switch to the region which contains your production setup</li>
<li>Go to <code>Systems Manager</code> and click on <code>Parameter Store</code></li>
<li>Click on <code>Create Parameter</code></li>
<li>Add <code>Name</code> and <code>Description</code></li>
<li>Select <code>Tier</code> as <code>Standard</code></li>
<li>Select <code>Type</code> as <code>SecureString</code></li>
<li>Select KMS key which managed encryption as per your choice</li>
<li>Enter entire <code>.env</code> contents into the text-area</li>
<li>Click on <code>Create Parameter</code> to save the parameter</li>
</ol>

<p><img src="/images/aws-ansible/create-parameter.png" alt="alt text1" /></p>

<h3 id="accessing-the-paremeter-store-values-%3A">Accessing the paremeter store values :</h3>

<p>Your EC2 instance which runs the application will be accessing these parameter store values. So you need to make sure your instance IAM role has following  permission in its policy.</p>

<pre><code class="json">{
    "Version": "2012-10-17",
            "Statement": [
        {
            "Sid": "VisualEditor1",
            "Effect": "Allow",
            "Action": [
                "ssm:GetParameters",
                "ssm:GetParameter",
            ],
            "Resource": [
                "arn:aws:ssm:*:*:parameter/*"
            ]
        }
    ]
}
</code></pre>

<h3 id="shell-script-to-generate-.env-%3A">Shell script to generate .env :</h3>

<p>We will be using following shell script to generate <code>.env</code> :</p>

<pre><code class="bash">#!/bin/bash

# Please update below varoables as per your production setup
PARAMATER="APP_ENV"
REGION="us-west-1"
WEB_DIR="/var/www/html"
WEB_USER="www-data"

# Get parameters and put it into .env file inside application root
aws ssm get-parameter --with-decryption --name $PARAMATER --region $REGION | jq '.Parameter.Value' &gt; $WEB_DIR/.env

# Clear laravel configuration cache
cd $WEB_DIR
chown $WEB_USER. .env
sudo -u $WEB_USER php artisan config:clear
</code></pre>

<h3 id="putting-pieces-together-%3A">Putting pieces together :</h3>

<p>Let us now consider a very generic overview of how these pieces will fit together :</p>

<ul>
<li>You will have your <code>.env</code> file stored in AWS systems manager parameter store with encryption enabled at rest</li>
<li>When a new instance will spin up, it will pull the env string and put into an <code>.env</code> file. This will happen using <code>user_data</code> setting</li>
<li>It will clear the application env cache or any dependent commands to take new environmental file into effect</li>
</ul>

<h3 id="fundamental-implementation-situations-%3A">Fundamental implementation situations :</h3>

<p>Let's say you have added a new variable in your env paramater group. You expect it to reflect ASAP in following situations :</p>

<ul>
<li><strong>All the new EC2 instances which will spin up after that point should always use the new env file variables</strong> :</li>
</ul>

<p>This can be easily implemented as we discussed earlier using <code>user_data</code>. When you spin up an instance manually you can add the above shell script to pull latest .env from parameter group in the <code>user_data</code> field.</p>

<p>If you have auto-scaling enabled, make sure your launch configuration or launch template has the <code>user_data</code> field set with the shell script we discussed in the previous section. This will then make sure, whenever a new EC2 instance will spin up, it will always fetch the environmental paramater first from AWS systems manager's parameter group store and then make a branch new <code>.env</code> file.</p>

<ul>
<li><strong>The existing instances which are already running should fetch the updated env file variables</strong> :</li>
</ul>

<p>Let's first take out the case where you have instance not in any auto-scaling group. You can then manually SSH into instance and fetch the new .env. (Or automate it using Solution B)</p>

<p>But in here main point of concern is when you have autoscaling setup and production is running on multiple instances. This is an area where we can have 2 solutions. First one is very easy to implement whereas second one can be a pain. Let's discuss both :</p>

<p><code>Solution A</code> :</p>

<p>From your existing auto-scaling instances, start terminating an instance one by one. Your auto-scaling setup will then spin up new instances. As we discussed earlier, considering your launch configuration already has the shell script to pull updated environment variables from parameter group at the instance start from <code>user_data</code> setting, the new instances will be ready with updated environmental veriables in their <code>.env</code> files.</p>

<p>To add more automation, you may also do the termination activity using a lambda function or ansible based on a trigger when systems manager's parameter group is updated.</p>

<p><code>Solution B</code> :</p>

<p>This is important for applications where there ephemeral storage is important for some ongoing tasks. If we terminate the instances, the production application will lose some of it's ongoing process data. In this case we need to find an automated way to run the shell script on all those running instances, to update their <code>.env</code> variables.</p>

<p>In this case, you can use AWS lambda for this purpose. The bottom line of this is <code>When systems manager's parameter group is updated, it will trigger a lambda function. That lambda function will SSH into all your running production instances and update the .env file</code>.</p>

<p>There is already a github repo I have added sometime back on running SSH commands on EC2 instances. You can <a href="https://github.com/techsemicolon/ec2-run-commands-log-output">click here</a> to know more about it.</p>

<h3 id="my-two-cents-%3A">My two cents :</h3>

<p>This may definitely appear as an overhead... But as your application becomes large, setting these things up will make your life so much easier.</p>
]]></content>
        </entry>
            <entry>
            <title type="html"><![CDATA[Ansible everything you need to know about set_facts]]></title>
            <link href="https://techsemicolon.github.io/blog/2019/07/07/ansible-everything-you-need-to-know-about-set-facts/"/>
            <updated>2019-07-07T00:00:00+00:00</updated>
            <id>https://techsemicolon.github.io/blog/2019/07/07/ansible-everything-you-need-to-know-about-set-facts/</id>
            <content type="html"><![CDATA[<p>If you have seen lot of ansible playbook examples, <code>set_facts</code> module is very common in most of them. Let us dive little deeper to know what is it and how it may help you to write dynamic playbooks.</p>

<h3 id="the-jargon-%60set_facts%60--%3A">The jargon <code>set_facts</code>  :</h3>

<p>If you just read <code>set_facts</code> in an ansible playbook, it is really hard to interpret what it really means. You may think like it is setting some kind of facts but what facts? I had the same doubt and I was little overwhelmed by the terminology in here. But, to understand in generic terms :</p>

<p><code>set_facts</code> module sets variables once you know their values and optionally deciding if you need their values to be set or not.</p>

<p>You may set simple variables in ansible using <code>vars</code>, <code>vars_file</code> or <code>include_vars</code> modules, however you know their values beforehand. In case of <code>set_facts</code>, you set variables <code>on the fly</code> depending on certain task results. These variables will be available to be used in the subsequent plays during an ansible playbook execution.</p>

<h3 id="let%27s-see-a-real-life-use-case-%3A">Let's see a real life use case :</h3>

<p>Before diving into an actual playbook and overal syntax, let's first take a real life use case, which will help us connect the dots.</p>

<p>Use case : We need to spin up an instance on AWS and then add it into an existing AWS Target Group.</p>

<p>Known Variables : We will have some known variables like the instance AMI id and the instance type.</p>

<p>Unkwnon Variables : To add an instance to target group, we need the instance id. However, until that instance is spun up, we will not have the instance id. This will be set on the fly once instance is spun up.</p>

<p>In this case, we will use known variables to spin up an instance. Once that task is done, we will get an instance id from AWS. Thats a <code>fact</code> which we have come across from that task. We will set it to a variable using <code>set_facts</code> module and then it can be used later to add instance into an existing AWS Target Group.</p>

<h3 id="register-and-set_facts-go-hand-in-hand-%3A">Register and set_facts go hand in hand :</h3>

<p>Until you have receieved a factual information or to be specific, a task result, you do not have facts to set using <code>set_facts</code> module. This is why I always feel <code>register</code> module and <code>set_facts</code> module go hand in hand.</p>

<p>Please note that, there are lots of other ways to get factual information from a task and <code>register</code> is not the only way. But it is one of the most common ways you would get facts. Some other modules to get factual information can be <code>ansible_facts</code> to get package information (package_facts) from a host. The possibilities are much more.</p>

<h3 id="set_facts-is-host-specific-%3A">set_facts is host specific :</h3>

<p>A very important thing to note that when you set a fact using <code>set_facts</code> module, it is specific to the host within which task is currently running. As documentation says : <code>Variables are set on a host-by-host basis just like facts discovered by the setup module.</code> If your playbook has multiple hosts then you can not share a fact set using <code>set_facts</code> from one host to another.</p>

<h3 id="diving-into-an-example-%3A">Diving into an example :</h3>

<p>Let's take the use case we discussed earlier and make a simple playbook for it. We will have following structure :</p>

<pre><code>ReleaseAMIUpdates/
├── config.yml
├── env.yml
├── playbook.yml
└── setup.sh
</code></pre>

<p>Please note that you may have more detailed structure based on your preferences. This article is about exploring <code>set_facts</code>, so we will focus more on its implementation.</p>

<ul>
<li>config.yml :</li>
</ul>

<p>This file will have the configuration variables which rarely change.</p>

<pre><code class="yml">vpc_id: vpc-12345678
ec2_iam_role: ec2-iam-role-name
instance_type: t2.micro
instance_volume_in_gb: 30
instance_security_group: ec2-security-group-name
ami_id: ami-12345678
instance_key_name: ansible-instance.pem
</code></pre>

<ul>
<li>env.yml :</li>
</ul>

<p>This file will have the configurations which are sensitive and may change .</p>

<pre><code class="yml">region: us-west-1
aws_access_key: your_aws_access_key
aws_secret_key: your_aws_secret_key
target_group: Test Ansible Target Group
</code></pre>

<ul>
<li>setup.sh :</li>
</ul>

<p>This file will have any user-data boostrap commands you need to run as soon as new instance is spun up. We will use this to install nginx so that we can server web traffic with a simple web page.</p>

<pre><code class="bash">#!/bin/bash

# Update Package Lists
apt-get update

# Install add-apt-repository dependencies
apt-get install software-properties-common -y
apt-get install python-software-properties -y

# Update Package Lists
apt-get update -y

# Install nginx 
apt-get -y install nginx
</code></pre>

<p>Now you have above yml files set up, these files will act as environment variable files. We will refer the configurations specified above as variables in our playbook.</p>

<ul>
<li>playbook.yml :</li>
</ul>

<p>This file will contain all palybook tasks.
</p>

<pre><code class="yaml"># create a launch configuration using an AMI image and instance type as a basis
- name: Launch new AMI Release
  hosts: localhost
  connection: local
  vars_files:
    - ./env.yml
    - ./config.yml

  tasks:

  #  Get VPC public subnet details as it will be needed later while launching the sandbox instance
  - name: Get VPC Subnet Details
    ec2_vpc_subnet_facts:
      aws_access_key: "{{ aws_access_key }}"
      aws_secret_key: "{{ aws_secret_key }}"
      region: "{{ region }}"
      filters:
        vpc-id: "{{ vpc_id }}"
        "tag:Availability": "Public"
    # Save the result json in variable subnet_facts_public
    register: subnet_facts_public

  - name: Get VPC Subnet ids which are available and public
    set_fact:
      vpc_subnet_id_public: "{{ subnet_facts_public.subnets|selectattr('state', 'equalto', 'available')|map(attribute='id')|list|random }}"

  # Launch instance with required settings
  - name: Launch new instance
    ec2:
      key_name: "{{ instance_key_name }}"
      aws_access_key: "{{ aws_access_key }}"
      aws_secret_key: "{{ aws_secret_key }}"
      region: "{{ region }}"
      image: "{{ ami_id }}"
      instance_profile_name: "{{ ec2_iam_role }}"
      vpc_subnet_id: "{{ vpc_subnet_id_public }}"
      instance_type: "{{ instance_type }}"
      group: "{{ instance_security_group }}"
      assign_public_ip: False
      # All commands specified in below will run as soon as instance is launched
      user_data: "{{ lookup('file', 'setup.sh') }}"
      wait: True
      wait_timeout: 500
      volumes: 
        - device_name: /dev/sda1
          volume_size: "{{ instance_volume_in_gb }}"
          volume_type: gp2
          encrypted: True
          delete_on_termination: True
      instance_tags:
        Name: Ansible-Test-Instance
    register: ec2

  # Get instance id from registered facts in ec2
  - name: Get instance id from registered facts in ec2
    set_fact:
      new_instance_id: "{{ ec2.instance_ids[0] }}"

  # Add newly created instance into target group
  - name: Add newly created instance into target group
    elb_target_group:
      name: "{{ target_group }}"
      aws_access_key: "{{ aws_access_key }}"
      aws_secret_key: "{{ aws_secret_key }}"
      region: "{{ region }}"
      target_type: instance
      health_check_interval: 30
      health_check_path: /health
      health_check_protocol: http
      health_check_timeout: 15
      healthy_threshold_count: 2
      unhealthy_threshold_count: 2
      protocol: http
      port: 80
      vpc_id: "{{ _vpc_id }}"
      successful_response_codes: "200"
      targets:
        - Id: "{{ new_instance_id }}"
          Port: 80
      state: present

</code></pre>

<p>
Lets walk through each task in above <code>playbook.yml</code> first before we run it :</p>

<ol>
<li>We already know our vpc id. But to spin up an instance, we will need to get subnet id. We will use ansible module <code>ec2_vpc_subnet_facts</code> to get the public subnets. We will register that result into <code>subnet_facts_public</code> variable.</li>
<li>We will use <code>subnet_facts_public</code> variable to parse its content and get a public subnet which is available chosen randomly from set of available public subnets from the results. The type of parsing used is called <code>jinja</code> which comes within ansible. Once we have that fact, we will set it using <code>set_facts</code> into <code>vpc_subnet_id_public</code> variable on the fly.</li>
<li>We will launch new instance and then get its information. We will register that into <code>ec2</code> variable.</li>
<li>We will use <code>ec2</code> variable to parse its content and get the instance id of newly spun up instance. Once we have that fact, we will set it using <code>set_facts</code> into <code>new_instance_id</code> variable on the fly.</li>
<li>Finally we will update our target group and add this instance into its targets.</li>
</ol>

<h3 id="conditionally-set-facts-%3A">Conditionally set facts :</h3>

<p>You might need to set a fact using <code>set_facts</code> module when another variable or result registered contains some dependent value. In such case you can use <code>when</code> conditional.</p>

<p>Example :</p>

<p></p>

<pre><code class="yaml">- name: Get VPC Subnet ids which are available and public
    set_fact:
      vpc_subnet_id_public: "{{ subnet_facts_public.subnets|selectattr('state', 'equalto', 'available')|map(attribute='id')|list|random }}"
    when: region == "us-west-2" 
</code></pre>

<p></p>

<h3 id="caching-a-set-fact-%3A">Caching a set fact :</h3>

<p>You can cache a fact set from <code>set_facts</code> module so that when you execute your playbook next time, it's retrieved from cache. You can set <code>cacheable</code> to <code>yes</code> to store variables across your playbook executions using a fact cache. You may need to look into precedence strategies used by ansible to evaluate the cacheable facts mentioned in their <a href="https://docs.ansible.com/ansible/latest/modules/set_fact_module.html?highlight=set_facts">documentation</a>.</p>
]]></content>
        </entry>
            <entry>
            <title type="html"><![CDATA[Ansible AWS rolling AMI update with zero downtime]]></title>
            <link href="https://techsemicolon.github.io/blog/2019/07/01/ansible-aws-rolling-ami-update-with-zero-downtime/"/>
            <updated>2019-07-01T00:00:00+00:00</updated>
            <id>https://techsemicolon.github.io/blog/2019/07/01/ansible-aws-rolling-ami-update-with-zero-downtime/</id>
            <content type="html"><![CDATA[<p>If you have website hosted on AWS with an Auto Scaling enabled, doing AMI rolling updates manually is a pain. But ansible makes it so much easy for you. Let's understand how you can save time and efforts for AMI rolling updates with zero downtime.</p>

<h3 id="what-is-a-rolling-ami-update-%3A">What is a rolling AMI update :</h3>

<p>When you have Auto Scaling enabled, AWS will scale up and down your setup by increasing or decreasing number of instances automatically based on server load and your auto scaling policies. AWS uses an instance template called <code>Launch Configuration</code> using which it understands what AMI to use when spinning up new instances automatically to scale up.</p>

<p>Now, lets assume that you have 4 instances currently in-service associated with your auto scaing with their AMI version as <code>V1</code>. Now you need to release a new AMI version <code>V2</code>. What you will ideally do is :</p>

<ul>
<li>Create a new launch configuration which points to new AMI version V2. To do it manually you will basicaly <code>copy</code> your existing launch configuration and update AMI id.</li>
<li>Edit your Auto Scaling group and associate it with newly created launch configuration.</li>
<li>By just doing above steps will not update the existing in-service instances. You will terminate the existing in-service instances one by one. Once an instance inside auto-scaling in-service listeners is terminated, auto scaling group will launch a new one to keep minimum number of instances in-service as per the auto scaling policy.</li>
<li>This new instance will now be from AMI version V2</li>
</ul>

<p>This is a rolling update, which most of the times is done manually. It takes approx 10-15 minutes to do it manually. Let's understand how you can do it under 2-3 minutes with ansible with 2-3 minutes rollback with just one configuration change.</p>

<h3 id="prerequisites-%3A">Prerequisites :</h3>

<p>You will need following before you start working on ansible playbook and it's tasks :</p>

<ul>
<li>You will need <code>ansible 2.8.x</code> and <code>boto3</code> installed on the system. Preferred way to install these is using <code>pip</code> installer.</li>
<li>You will need an AWS CLI user with access key and secret access key. I always prefer doing this in a non-production region first so that if you mess up anything, there is minimum worry. Let's say your production AWS region is <code>us-west-1</code> then you woul setup a clone in <code>us-west-2</code> and then test ansible playbook in there. You can use below IAM policy for the CLI user</li>
</ul>

<pre><code class="json">{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "ec2:*",
                "rds:*",
                "lambda:*",
                "autoscaling:*",
                "iam:PassRole",
                "elasticloadbalancing:*"
            ],
            "Resource": "*",
            "Condition": {
                "StringEquals": {
                    "aws:RequestedRegion": "us-west-2"
                }
            }
        }
    ]
}
</code></pre>

<h3 id="setup-%3A">Setup :</h3>

<p>We will have following structure :</p>

<pre><code>ReleaseAMIUpdates/
├── config.yml
├── env.yml
├── playbook.yml
└── startup.sh
</code></pre>

<ul>
<li>config.yml :</li>
</ul>

<p>This file will have the configuration variables which rarely change.</p>

<pre><code class="yml">project_lunch_config_name: Production Launch configuration
project_autoscaling_group_name: Prod Auto Scaling Group
project_vpc_id: vpc-12345678
project_ec2_iam_role: ec2-iam-role-name
project_instance_type: t2.micro
project_instance_volume_in_gb: 30
project_instance_security_group: ec2-security-group-name
</code></pre>

<ul>
<li>env.yml :</li>
</ul>

<p>This file will have the configurations which are sensitive and may change in each rolling update.</p>

<pre><code class="yml">project_region: us-west-1
project_aws_access_key: your_aws_access_key
project_aws_secret_key: your_aws_secret_key
project_golden_ami_id: ami-version-id
project_ami_version: V2
project_target_group_arn: arn:aws:elasticloadbalancing:us-west-2:123456789:targetgroup/Your-TargetGroup/233vc4441187369
</code></pre>

<ul>
<li>startup.sh :</li>
</ul>

<p>This file will have any user-data boostrap commands you need to run as soon as new instance is spun up.</p>

<pre><code class="bash">#!/bin/bash

# Add your commands here
# These will run as root
# Which means ~ refers to /root
</code></pre>

<p>Now you have above yml files set up, these files will act as environment variable files. We will refer the configurations specified above as variables in our playbook.</p>

<ul>
<li>playbook.yml :</li>
</ul>

<p>This file will contain all palybook tasks.
</p>

<pre><code class="yaml"># create a launch configuration using an AMI image and instance type as a basis
- name: Launch new AMI Release
  hosts: localhost
  connection: local
  vars_files:
    - ./env.yml
    - ./config.yml

  tasks:

  #  Get VPC subnet details as it will be needed later while setting up autoscaling group
  - name: Get VPC Subnet Details
    ec2_vpc_subnet_facts:
      aws_access_key: "{{ project_aws_access_key }}"
      aws_secret_key: "{{ project_aws_secret_key }}"
      region: "{{ project_region }}"
      filters:
        vpc-id: "{{ project_vpc_id }}"
        "tag:Availability": "Private"
    # Save the result json in variable subnet_facts
    register: subnet_facts

  # From the previously registered variable subnet_facts
  # Get filter subnet which are in avaible state
  # Use jinja to parse json and get list of ids
  # This list will be used directly while setting up autoscaling group
  - name: Get VPC Subnet ids which are available
    set_fact:
      vpc_subnet_ids: "{{ subnet_facts.subnets|selectattr('state', 'equalto', 'available')|map(attribute='id')|list }}"

  # Create new launch configuration as existing one can not be edited
  # This launch configuration will contain the new AMI
  - name: Configure new launch configuration
    ec2_lc:
      aws_access_key: "{{ project_aws_access_key }}"
      aws_secret_key: "{{ project_aws_secret_key }}"
      region: "{{ project_region }}"
      name: "{{ project_lunch_config_name }}"
      # This image Id will be the new golden AMI after release is complete
      image_id: "{{ project_golden_ami_id }}"
      instance_profile_name: "{{ project_ec2_iam_role }}"
      vpc_id: "{{ project_vpc_id }}"
      security_groups: ["{{ project_instance_security_group }}"]
      instance_type: "{{ project_instance_type }}"
      # All commands specified in below ./startup.sh will run as soon as instance is launched
      user_data_path: ./startup.sh
      volumes:
      - device_name: /dev/sda1
        volume_size: "{{ project_instance_volume_in_gb }}"
        volume_type: gp2
        iops: 3000
        delete_on_termination: true
        encrypted: true

  # Update autoscaling group and associate new launch configuration
  # As there is no AMI just to update an existing autoscaling group
  # We specify all options and ansible will match the name to update
  - name: Update Auto Scalling Group with new launch configuration
    ec2_asg:
      aws_access_key: "{{ project_aws_access_key }}"
      aws_secret_key: "{{ project_aws_secret_key }}"
      name: "{{ project_autoscaling_group_name }}"
      region: "{{ project_region }}"
      launch_config_name: "{{ project_lunch_config_name }}"
      default_cooldown: 180
      health_check_period: 300
      health_check_type: ELB
      target_group_arns: ["{{ project_target_group_arn }}"]
      desired_capacity: 4
      min_size: 4
      max_size: 6
      vpc_zone_identifier: "{{ vpc_subnet_ids }}"
      # Below settings will replace all existing instances in this autoscaling group
      # With instances of new AMI release
      # The replacing will happen in batches with 2 instances replaced at at time
      replace_all_instances: true
      replace_batch_size: 2
      # We will wait untill all newly replaced instances are healthy and in service
      # Max wait time will be 10 minutes after which ansible will time out
      # In case of timeout the activity will keep happening on AWS
      # Just that the terminal will not wait for the output and exit with code 0
      wait_for_instances: true
      wait_timeout: 600
      # Below tabs will be present on all production instances launched with new AMI
      tags:
      - Environment: Production
        Name : "Production instances | {{ project_ami_version }}"
        Project: Your Project Name
        Vesion : "{{ project_ami_version }}"
</code></pre>

<p>
Lets walk through each task in above <code>playbook.yml</code> first before we run it :</p>

<ol>
<li><p>Get VPC Subnet Details : The instances will be launched in a VPC. We will need the subnet ids from the VPC we will need instances to be present in. In here I have used a tag <code>Availability:Private</code> to only get private instances as in my setup, instances are not publically accessible from a public subnet.</p></li>
<li><p>Get VPC Subnet ids which are available : The 1st task will give us entire json details of VPC subnets. This task will filter and get only ids using jinja parsing of the json result.</p></li>
<li><p>Configure new launch configuration : We will create a new launch configuration. I have been very descriptive in the options used in this task to make sure it's easy to refer next time.</p></li>
<li><p>Update Auto Scalling Group with new launch configuration : This will associate the new launch configuration to an existing auto scaling group. Make sure your auto scaling group name matches to the one present already so that it's updated properly. The <code>replace_all_instances: true</code> makes sure we are rolling the new AMIs instantly. This task will wait for the instances to spin up and be <code>in-service</code> state. This is sepcified by <code>wait_for_instances</code> and <code>wait_timeout</code> options in this task.</p></li>
</ol>

<h3 id="running-the-playbook-%3A">Running the playbook :</h3>

<p>First step is to make sure you have correct variables set in the <code>env.yml</code> and <code>config.yml</code>. When you do it for the second time, you will just need to change <code>project_golden_ami_id</code> and <code>project_ami_version</code> variables.</p>

<p>Before running it directly, it's always safe to run it using <code>--check</code> mode as a dryrun, with -vv to have more verbose output :</p>

<pre><code class="bash">ansible-playbook playbook.yml -vv --check
</code></pre>

<p>Deploying the new AMI :</p>

<pre><code class="bash">ansible-playbook playbook.yml -vv
</code></pre>

<h3 id="rolling-back-the-update-%3A">Rolling back the update :</h3>

<p>If your AMI which was newly released had issues, you can easily roll it back by specifying old stable values in <code>project_golden_ami_id</code> and <code>project_ami_version</code> variables. Then you just need to deploy the playbook.</p>

<h3 id="why-to-invest-time-in-ansible-%3A">Why to invest time in ansible :</h3>

<p>As your AWS setup grows, the manual activities which were simple at first become start becoming an overhead. Plus, there is always a risk of errors when manual operations are concerned. Using an automation tool like ansible lets you do the same actions with 70-80% less time than you would need to do it manually. Also ansible playbooks become a reference documentation if you need to explain anyone from your team how AMI updates are performed.</p>

<h3 id="tracking-ansible-playbooks-in-git-repo-%3A">Tracking ansible playbooks in git repo :</h3>

<p>If you want to track these ansible playbooks in git, make sure you do not track the main <code>env.yml</code> file which has AWS CLI crednetials. That is why we have 2 separate files <code>env.yml</code> and <code>config.yml</code>.</p>

<h3 id="improvements-%3A">Improvements :</h3>

<p>If you would like, you can update the AWS CLI user IAM policy to add more granuler permissions which is always preferable.</p>
]]></content>
        </entry>
            <entry>
            <title type="html"><![CDATA[Laravel APP_KEY rotation policy for app security]]></title>
            <link href="https://techsemicolon.github.io/blog/2019/06/14/laravel-app-key-rotation-policy-for-security/"/>
            <updated>2019-06-14T00:00:00+00:00</updated>
            <id>https://techsemicolon.github.io/blog/2019/06/14/laravel-app-key-rotation-policy-for-security/</id>
            <content type="html"><![CDATA[<p>If you have an existing laravel app running or you do fresh laravel installation, you will notice in your app's <code>.env</code> file (in case of new installations it's in <code>.env.example</code> file), there is a key called <code>APP_KEY</code>. It is a  32 characters long string. This is the laravel application key.</p>

<p>Laravel has an artisan command <code>php artisan key:generate</code> which generates a new <code>APP_KEY</code> for you. If you have installed Laravel via Composer or the Laravel installer, this key will be set for you automatically by the <code>key:generate</code> command.</p>

<h4 id="why-your-app-key-is-important%3F">Why your app key is important?</h4>

<p>The <code>APP_KEY</code> is used to keep your user sessions and other encrypted data secure! If the application key is not set, your user sessions and other encrypted data will not be secure. Believe it or not it is a big security risk.</p>

<p>To give you more specific context, earlier laravel had a security issue :</p>

<p><code>If your application's encryption key is in the hands of a malicious party, that party could craft cookie values using the encryption key and exploit vulnerabilities inherent to PHP object serialization / unserialization, such as calling arbitrary class methods within your application.</code></p>

<p>Do not worry, laravel later released a security update which disabled all serialization and unserialization of cookie values using <code>APP_KEY</code>. All Laravel cookies are encrypted and signed, cookie values can be considered safe from client tampering. <a href="https://laravel.com/docs/5.6/upgrade#upgrade-5.6.30"><code>clich here</code></a> to see details about this security update.</p>

<p>Before Update :</p>

<p>If used to serialize all values during encryption.</p>

<pre><code class="php">$encryptedValue = \openssl_encrypt(serialize($value), $this-&gt;cipher, $this-&gt;key, 0, $iv);
</code></pre>

<p>After Update :</p>

<p>It will serialize only if you pass second parameter as <code>true</code> while calling encrypt function.</p>

<pre><code class="php">$encryptedValue = \openssl_encrypt(
    $serialize ? serialize($value) : $value,
    $this-&gt;cipher, $this-&gt;key, 0, $iv
);
</code></pre>

<p>Okay, to sum it up, <code>APP_KEY</code> is important and any backdoor associated with it which leads to compromising app security should be closed.</p>

<h4 id="passwords-and-app_key-%3A">Passwords and APP_KEY :</h4>

<p>When we consider parts of the app which have encryption, first thing comes to mind is the user passwords. Let's take a minute to differentiate both :</p>

<ul>
<li>Encryption :</li>
</ul>

<p>Encryption is when you have a data which you want to safeguard. So you take original data, encrypt it using key and ciphers so it turns into gibrish string. This gribrish string has no meaning and hence it can not be interpreter directly toits original data meaning. When you need, you can decrypt this encrypted value to retrive it in original state.</p>

<p>Laravel has <code>Crypt</code> facade which helps us implement encryption and decryption.</p>

<p>In this case, the <code>key</code> and <code>ciphers</code> are important because those are used in decryption. And should NOT be explosed.</p>

<ul>
<li>Hashing :</li>
</ul>

<p>Hashing in simple terms is one way encryption. Once you encrypt you can NOT decrypt it in original state. You can verify if the hash matches a plain value to check if its the original value or not. But that's all you can do. It is way more secure in terms of sensitive information like user passwords.</p>

<p>Laravel has <code>Hash</code> facade which helps us implement one way hashing encryption.</p>

<p>Now, one of the main thing you should understand : APP_KEY is <code>NOT</code> used in Hashing and it is used in encryption. So your passwords security is NOT dependent on the APP_KEY. Whereas any data of your app which you have encrypted <code>do have</code> dependency on APP_KEY.</p>

<h4 id="affects-of-changing-app_key-%3A">Affects of changing APP_KEY :</h4>

<p>Before we dive into how to change APP_KEY, it is important to know what will happen if we change it. When APP_KEY is changed in an existing app :</p>

<ul>
<li>Existing user sessions will be invalidated. Hence, all your currently active logged in users will be logged out.</li>
<li>Any data in your app which you have encrypted using <code>Crypt</code> facade or <code>encrypt()</code> helper function will no longer be decrypted as the encryption uses APP_KEY.</li>
<li>Any user passwords will NOT be affected so no need to worry about that.</li>
<li>If you have more than one app servers running, all should be replaced with the same APP_KEY.</li>
</ul>

<h4 id="let%27s-handle-the-headache-first-%3A">Let's handle the headache first :</h4>

<p>As seen above, you can imagine most headache in changing APP_KEY is handling the data which is encrypted using old app key. For that you need to first decrypt the data using old APP_KEY and then re-encrypt using new APP_KEY. Damn!</p>

<p>Don't worry! Here is a simple helper function you can use :</p>

<pre><code class="php">/**
 * Function to re-encrypt when APP_KEY is rotated/changed
 * 
 * @param string $oldAppKey
 * @param mixed $value
 */
function reEncrypt($oldAppKey, $value)
{
    // Get cipher of encryption
    $cipher = config('app.cipher');
    // Get newly generated app_key
    $newAppKey = config('app.key');

    // Verify old app Key
    if (Str::startsWith($oldAppKey, 'base64:')) {
        $oldAppKey = base64_decode(substr($oldAppKey, 7));
    }
    // Verify new app Key
    if (Str::startsWith($newAppKey, 'base64:')) {
        $newAppKey = base64_decode(substr($newAppKey, 7));
    }

    // Initialize encryptor instance for old app key
    $oldEncryptor = new Illuminate\Encryption\Encrypter((string)$oldAppKey, $cipher);

    // Initialize encryptor instance for new app key
    $newEncryptor = new Illuminate\Encryption\Encrypter((string)$newAppKey, $cipher);

    // Decrypt the value and re-encrypt
    return $newEncryptor-&gt;encrypt($oldEncryptor-&gt;decrypt($value));
}
</code></pre>

<p>Let's imagine we have a column called <code>bank_account_number</code> in <code>users</code> table which is stored as encrypted string. I have another column called <code>old_bank_account_number</code> in the <code>users</code> table to store old value as a backup before we save newly re-encrypted value. We can create a command <code>php artisan encryption:rotate</code> :</p>

<pre><code class="php">&lt;?php

namespace App\Console\Commands;

use App\User;
use Illuminate\Console\Command;
use Illuminate\Encryption\Encrypter;

class EncryptionRotateCommand extends Command
{
    /**
     * The name and signature of the console command.
     *
     * @var string
     */
    protected $signature = 'encryption:rotate {--oldappkey= : Old app key}';

    /**
     * The console command description.
     *
     * @var string
     */
    protected $description = 'Re-encrypt when APP_KEY is rotated';

    /**
     * Create a new command instance.
     *
     */
    public function __construct()
    {
        parent::__construct();
    }

    /**
     * Function to re-encrypt when APP_KEY is rotated/changed
     * 
     * @param string $oldAppKey
     * @param mixed $value
     */
    public function handle()
    {
        // Get the old app key
        $oldAppKey = $this-&gt;option('oldappkey');

        // Get cipher of encryption
        $cipher = config('app.cipher');
        // Get newly generated app_key
        $newAppKey = config('app.key');

        // Verify old app Key
        if (Str::startsWith($oldAppKey, 'base64:')) {
            $oldAppKey = base64_decode(substr($oldAppKey, 7));
        }
        // Verify new app Key
        if (Str::startsWith($newAppKey, 'base64:')) {
            $newAppKey = base64_decode(substr($newAppKey, 7));
        }

        // Initialize encryptor instance for old app key
        $oldEncryptor = new Encrypter((string)$oldAppKey, $cipher);

        // Initialize encryptor instance for new app key
        $newEncryptor = new Encrypter((string)$newAppKey, $cipher);

        User::all()-&gt;each(function($user) use($oldEncryptor, newEncryptor){

            // Stored value in a backup column
            $user-&gt;old_bank_account_number  = $user-&gt;bank_account_number;

            // Decrypt the value and re-encrypt
            $user-&gt;bank_account_number  = $newEncryptor-&gt;encrypt($oldEncryptor-&gt;decrypt($user-&gt;bank_account_number));
            $user-&gt;save();

        });

        $this-&gt;info('Encryption completed with newly rotated key');
    }
}
</code></pre>

<h3 id="update-%3A">Update :</h3>

<p>I have pushed a new package which helps to simplify above implementation. <a href="https://github.com/techsemicolon/laravel-app-key-rotation"><code>click here</code></a> to view the Laravel package.</p>

<h4 id="rotating-the-key-%3A">Rotating the key :</h4>

<p>Finally, lets rotate the key.</p>

<pre><code>- Run `php artisan down` on all instances so that no user can interact until this is done
- Go to terminal on one of the instance and open your `.env` file. 
- Copy the APP_KEY value which is the old(existing) app key
- Run `php artisan key:generate` which will generate a new APP_KEY
- Run the helper command as we created above to re-encrypt the values `php artisan encryption:rotate --oldappkey=your_old_app_key_value`
- Replace the APP_KEY key on all remaining instances
- Run `php artisan config:clear` on all instances
- Run `php artisan cache:clear` on all instances
- Run `php artisan view:clear` on all instances
- Run `php artisan view:clear` on all instances
- Run `php artisan up` on all instances
</code></pre>

<p>And.. It's done! You can do this based on key rotation frequency you are comfortable in. I would recommend at-least once in 3 months.</p>
]]></content>
        </entry>
            <entry>
            <title type="html"><![CDATA[AWS update AMI using systems manager automation]]></title>
            <link href="https://techsemicolon.github.io/blog/2019/06/10/aws-update-ami-systems-manager-automation/"/>
            <updated>2019-06-10T00:00:00+00:00</updated>
            <id>https://techsemicolon.github.io/blog/2019/06/10/aws-update-ami-systems-manager-automation/</id>
            <content type="html"><![CDATA[<h4 id="overview-%3A">Overview :</h4>

<p>Having am AMI(Amazon Machine Image) of the most stable production server is a very common practice. On top of that adding maintenance scripts, installing new softwares or patching hotfixes are some of the common actions we need to perform on AMI.</p>

<p>The general flow in simple terms is :</p>

<pre><code>- Take an existing AMI(Let's call it AMI-V1)
- Launch an instance originating from the AMI-V1
- SSH into the new instance
- Run maintenance or installation commands
- Create a new AMI(Let's call it AMI-V2)
- Terminate the tempararily launched instance
- Optionally if you have autoscaling group launch configurations, then update it's association with new AMI-V2
</code></pre>

<p>If you do all above steps manually, there is nothing wrong in it. However, AWS has a service called <code>Systems Manager</code> which has a sub-service <code>automation</code>. It can help you to automate and also optionally schedule these AMI updates automatically. Trust me! If you know how to do it once, you will never go back to manual updates.</p>

<h4 id="how-it-works-%3A">How it works :</h4>

<p>SSM's automation service has different types. Those are called <code>Executions</code>. When you add details on what and how to setup that execution, the entire thing is called <code>Execution Document</code>. Dont worry, it's not that important to remember these names as long as you know how to use it.</p>

<p>There are many different types of executions available in automation service. One which we are looking for is called <a href="https://eu-west-2.console.aws.amazon.com/systems-manager/documents/AWS-UpdateLinuxAmi/description?region=eu-west-2"><code>AWS-UpdateLinuxAmi</code></a>. This execution helps us to automate updates to AMI with Linux distribution packages and Amazon software.</p>

<p>When you create the execution to automate the AMI updates, you can specify which AMI to update, specify the script of commands to run and specify the new AMI name. There are more steps to it which we will cover in <code>Implementation</code> section down below.</p>

<h4 id="prerequisites-and-difficult-terms-explained-%3A">Prerequisites and difficult terms explained :</h4>

<p>Before starting actual implementation, let's take a minute to understand few terms which are little tricky. I am very sure if any of you have dived into SSM automation for first time, you came across these and felt little overwhelmed.</p>

<p>When you set up the <code>AWS-UpdateLinuxAmi</code> automation, the form in AWS console asks you to give certain <a href="https://eu-west-2.console.aws.amazon.com/systems-manager/documents/AWS-UpdateLinuxAmi/parameters?region=eu-west-2"><code>parameters</code></a>(details of the setup)</p>

<p>The 2 parameters <code>IamInstanceProfileName</code> and <code>AutomationAssumeRole</code> are tricky to understand.</p>

<ul>
<li>IamInstanceProfileName :</li>
</ul>

<p>It's also referred as <code>ManagedInstanceProfile</code>. When AWS SSM runs the automation execution <code>AWS-UpdateLinuxAmi</code>, SSM needs to take certain actions in EC2 service. These are launching an instance from an existing source AMI, creating a new AMI from updated instance state and terminating the temparary instance etc.</p>

<p>For this purpose you create a new <code>IAM Role</code> (Identity Access Manamegement Role). The <code>IamInstanceProfileName</code> is nothing but the name of this created <code>IAM Role</code>.</p>

<ul>
<li>AutomationServiceRole :</li>
</ul>

<p>When SSM runs the automation execution <code>AWS-UpdateLinuxAmi</code>, SSM needs to assume the IAM role which we created for <code>IamInstanceProfileName</code> above, pass it as allowed so that it can function with the respective policies it's attached with.</p>

<p>For this purpose you create second a new <code>IAM Role</code> (Identity Access Manamegement Role). The <code>AutomationServiceRole</code> is nothing but the arn(Amazon Resource Name) of this created <code>IAM Role</code>.</p>

<ul>
<li>Trust Relationships :</li>
</ul>

<p>When you create both these roles mentioned above(We will dived in detail below), we need to update their <code>Trust Relationships</code>. With IAM roles, you can establish mutual trust relationships between your trusting account. In short, the trusting account owns the resource to be accessed and the trusted account contains the users who need access to the resource.</p>

<h4 id="creating-required-iam-roles-%3A">Creating required IAM roles :</h4>

<h5 id="step-1-%3A-create-iam-role-for-%2Aiaminstanceprofilename%2A-%3A">Step 1 : Create IAM Role for <em>IamInstanceProfileName</em> :</h5>

<pre><code>- Login to AWS console and go to `IAM` service. 
- Go to Roles listing and click on `Create Role`
- Keep type selected as `AWS Service`
- Below in `Choose the service that will use this role` section select `EC2`
- Click on `Next : Permissions` button on bottom right
- It will bring the page to attach permission policies
- Select `AmazonEC2RoleforSSM` policy
- Click on `Next : Tags` button on bottom right
- Add tags if you want or skip to next step by clicking `Next : Review` button on bottom right
- It will bring page to add role details
- Add a `Role name` as `ManagedInstanceProfileForSSM`
- Click on `Create Role`
</code></pre>

<h5 id="step-2-%3A-create-iam-role-for-%2Aautomationservicerole%2A-%3A">Step 2 : Create IAM Role for <em>AutomationServiceRole</em> :</h5>

<pre><code>- Login to AWS console and go to `IAM` service. 
- Go to Roles listing and click on `Create Role`
- Keep type selected as `AWS Service`
- Below in `Choose the service that will use this role` section select `EC2`
- Click on `Next : Permissions` button on bottom right
- It will bring the page to attach permission policies
- Select `AmazonSSMAutomationRole` policy
- Click on `Next : Tags` button on bottom right
- Add tags if you want or skip to next step by clicking `Next : Review` button on bottom right
- It will bring page to add role details
- Add a `Role name` as `AutomationServiceRole`
- Click on `Create Role`
</code></pre>

<h5 id="step-3-%3A-get-the-arn-string-of-%2Amanagedinstanceprofileforssm%2A-%3A">Step 3 : Get the arn string of <em>ManagedInstanceProfileForSSM</em> :</h5>

<p>We need to associate the first cerated IAM role <code>IamInstanceProfileName</code> to pass in this role.</p>

<pre><code>- Go to Roles listing and select `ManagedInstanceProfileForSSM`
- It will show the details of this role
- Copy the `arn` of this role 
- The arn will be in a format `arn:aws:iam::{IAM_USER_ID}:role/ManagedInstanceProfileForSSM`
- Keep this arn string somewhere as we will need it in next steps
</code></pre>

<h5 id="step-4-%3A-create-policy-to-pass-the-above-arn-into-second-role-%3A">Step 4 : Create policy to pass the above arn into second role :</h5>

<pre><code>- Login to AWS console and go to `IAM` service. 
- Go to `Policies` listing and click on `Create Ppolicy`
- It will show you visual editor and JSON tabs
- Click on `JSON` tab and paste following policy json
</code></pre>

<pre><code class="json">{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "VisualEditor0",
            "Effect": "Allow",
            "Action": "iam:PassRole",
            "Resource": "arn:aws:iam::{IAM_USER_ID}:role/ManagedInstanceProfileForSSM"
        }
    ]
}
</code></pre>

<pre><code>- Make sure you replace the resource arn in this policy json with the one you copied just few steps back
- Click on `Review Policy` button on bottom right 
- Give policy name as `PassPolicyForIamInstanceProfileRole` and create the policy
</code></pre>

<h5 id="step-5-%3A-attach-policy-to-pass-the-above-arn-into-second-role-%3A">Step 5 : Attach policy to pass the above arn into second role :</h5>

<pre><code>- Login to AWS console and go to `IAM` service.
- Go to Roles listing and select `AutomationServiceRole`
- Click on `Attach Policies` and attach `PassPolicyForIamInstanceProfileRole` policy
</code></pre>

<h5 id="step-5-%3A-update-trust-relationships-%3A">Step 5 : Update trust relationships :</h5>

<pre><code>- Login to AWS console and go to `IAM` service.
- Go to Roles listing and select `ManagedInstanceProfileForSSM`
- Go go `Trust Relationships` tab and click on `Edit trust relationship`
- Add following json and click on `Update trust relationship` in bottom right
</code></pre>

<pre><code class="json">{
    "Version": "2012-10-17",
    "Statement": [
        {
        "Effect": "Allow",
        "Principal": {
            "Service": [
            "ec2.amazonaws.com",
            "ssm.amazonaws.com"
            ]
        },
        "Action": "sts:AssumeRole"
        }
    ]
}
</code></pre>

<p>Hush!! Now the confusing part is DONE! We have few lats simple steps to complete.</p>

<h4 id="creating-script-of-commands-to-run-%3A">Creating script of commands to run :</h4>

<p>We need to create a script of commands which will run on the instance. This script needs to be stored on web and we just pass url of this script while configuring the automation execution.</p>

<p>For this, we will simply create an s3 bucket and store file in there instead.</p>

<pre><code>- Login to AWS console and go to `S3` service.
- Click on `Create Bucket`
- Give bucket a name and click on `Create`
- Create a script file on your local machine with name `ssm-automation.sh` and add below contents in it
</code></pre>

<pre><code class="bash">#!/bin/bash

# This script installs latex on the instance
# Feel free to change this as per your needs
sudo apt-get update
sudo apt-get install texlive-latex-base -y --allow-unauthenticated
</code></pre>

<pre><code>- Now upload this script file from your local machine to the new s3 bucket
- Once upload is completed, click on the file and click on `Make Public` so that it's publically accessible
- Copy its public url and save it somewhere as we will need it in next step
</code></pre>

<h4 id="the-moment-you-have-been-waiting-for-%3A">The moment you have been waiting for :</h4>

<p>Now its time to set up the final automation execution.</p>

<pre><code>- Login to AWS console and go to `Systems Manager` service.
- Click on `Automation`
- Click on `Execute Automation`
- Choose document page select `AWS-UpdateLinuxAmi`
</code></pre>

<p><img src="/images/aws-ssm-iam/select.png" alt="alt text" /></p>

<pre><code>- Click on `Next` on bottom right
- Add `SourceAmiId` as the AMI you want to update
- Add `IamInstanceProfileName` with value `ManagedInstanceProfileForSSM`
- In `AutomationAssumeRole` select `AutomationServiceRole`
- Update `TargetAmiName` with one you want
- Select instance type based on memory and space you need for your command executions
- Keep `SubnetId` blank if you do not want to launch the temparary instance in a specific VPC subnet
- Keep `PreUpdateScript` to `none`
- Update `PostUpdateScript` with public s3 script url
- Keep `IncludePackages` and `ExcludePackages` to `none`
- And finally... Click on `Execute`
</code></pre>

<p><img src="/images/aws-ssm-iam/setup.png" alt="alt text1" /></p>

<p>You will see progress of the automation with each step and it's details.</p>

<p><img src="/images/aws-ssm-iam/execution.png" alt="alt text" /></p>

<h4 id="some-suggestions-on-best-practices-%3A">Some suggestions on best practices :</h4>

<ol>
<li>If you have AWS instances setup in a custom VPC to comply with security regulations, make sure you select the VPC subnet ID so that the temparary instance will launch in that VPC. If not specified it launches instance in Default VPC</li>
<li>The script of commands you put in s3 should not contain any sensitive information. Instead, store it in AWS System manager parameter store. You need to make sure your EC2 instance profile has policy to access the parameter store.</li>
<li>Know what your commands do to guess how much memory and space and set the instance type accordingly while configuration automation execution.</li>
<li>Once you are comfortable with above basic setup, go ahead and study the execution document json. You can do lot of customization and actions like what happens if a step fails etc.</li>
</ol>

<h4 id="why-so-much-pain-%3A">Why so much pain :</h4>

<p>You might be thinking why go through so much pain of setup when manually you can do it in 15-20 minutes? Don't worry as it's a one time setup time. Next time all you need to do it update the AMI script in s3 and run the execution. Grab your coffee and relax when AWS creates a new update AMI version for you!</p>
]]></content>
        </entry>
            <entry>
            <title type="html"><![CDATA[Code diff checker tool online]]></title>
            <link href="https://techsemicolon.github.io/blog/2019/05/31/code-diff-online/"/>
            <updated>2019-05-31T00:00:00+00:00</updated>
            <id>https://techsemicolon.github.io/blog/2019/05/31/code-diff-online/</id>
            <content type="html"><![CDATA[<style type="text/css">
    .container {
        max-width : 90% !important;
    }
    textarea{
        resize: none;
        height: 800px;
    }
    .result-msg {
        display: none;
    }
    .deleted  {background-color : LightPink;
        text-decoration  : line-through}
    .inserted {background-color : PaleGreen}
    #difference {
        white-space:pre-wrap;
    }
    #check-difference {
            background: #4cc6b7;
        border-radius: 0;
        border: 0;
        margin: 25px 0;
        height: 50px;
        font-size: 25px;
    }
    textarea:focus, textarea:active {
        outline : 0;
        border:none;
    }
    textarea {
        border: none;
        width :100%;
        padding : 15px;
        padding: 15px;
        font-weight: 200;
        font-size: 16px;
        font-family: sans-serif;
        color: #4e6370;
    }
    .code-container {
        float:left;
        width:50%;
        border : 1px solid #627986;
    }
    .no-b-r {
        border-right : none;
    }
    .code-title {
        text-align: center;
        height: 50px;
        line-height: 50px;
        border-bottom: 1px solid #ddd;
    }
    #switch {
        background: #e9ece5;
        border-radius: 0;
        color: #4e6370;
        margin: 20px auto;
        width: 30%;
        border: 1px solid #ccc;
    }
    #reset {
        background: #e9ece5;
        border-radius: 0;
        color: #4e6370;
        margin: 20px auto;
        width: 30%;
        border: 1px solid #ccc;
    }
    #difference {
        white-space: pre-wrap;
        background: #fff;
        padding: 20px;
        width: 100%;
            font-weight: 200;
        font-size: 16px;
        font-family: sans-serif;
        color: #000;
        line-height: 20px;
    }
    .difference-container {
        display:none;
        width:100%;
        border: 1px solid #4e6370;
    }
</style>

<div class="alert alert-info result-msg">&nbsp;</div>

<div class="row">
    <button type="button" class="btn btn-primary btn-block" id="switch">< Switch ></button>
    <button type="button" class="btn btn-primary btn-block" id="reset">Reset</button>
</div>

<div class="row">
    <div class="code-container no-b-r">
        <div class="code-title">Original Code</div>
        <textarea spellcheck="false" id="left-code"></textarea>
    </div>
    <div class="code-container">
        <div class="code-title">Changed Code</div>
        <textarea spellcheck="false" id="right-code"></textarea>
    </div>
</div>

<div class="row">
    <button type="button" class="btn btn-primary btn-block" id="check-difference">Check Difference</button>
</div>

<div class="row">
    <div class="difference-container">
        <div class="code-title">Comparison Result</div>
        <div class="col-xs-12" id="difference"></div>
    </div>
</div>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.0/jquery.min.js"></script>

<script type="text/javascript">
jQuery(document).ready(function($){
    
    $('#switch').click(function(){
        var leftCode = $('#left-code').val();
        var rightCode = $('#right-code').val();
        $('#right-code').val(leftCode);
        $('#left-code').val(rightCode);
        $('#difference').html('');
        $('.difference-container').hide();
    });
    $('#reset').click(function(){
        $('#left-code').val('');
        $('#right-code').val('');
        $('#difference').html('');
        $('.difference-container').hide();
    });

    $('#check-difference').click(function(){
        var leftCode = clean($('#left-code').val());
        var rightCode = clean($('#right-code').val());

        if(leftCode == ''){
            alert('Please add value on the left hand side of comparison areas!');
            return;
        }
        if(rightCode == ''){
            alert('Please add value on the right hand side of comparison areas!');
            return;
        }

        compare(leftCode, rightCode);
    });

    function compare(leftCode, rightCode){

        if(leftCode === rightCode){
            setResult('It\'s a match! Both sides of comparison are identicle.');
            return;
        }

        $('#difference').html(leftCode.diff(rightCode));
        $('.difference-container').show();

    }

    function clean(code)
    {
        return code
         .replace(/&/g, "&amp;")
         .replace(/</g, "&lt;")
         .replace(/>/g, "&gt;")
         .replace(/"/g, "&quot;")
         .replace(/'/g, "&#039;");
    }

    String.prototype.largestMatch = function largestMatch(otherString) {

        if (otherString.length < this.length)
            return otherString.largestMatch(this);

        var matchingLength = otherString.length,
            possibleMatch, index;

        while (matchingLength) {
            index = 0;
            while (index + matchingLength <= otherString.length) {
                possibleMatch = otherString.substr(index, matchingLength);
                if (~this.indexOf(possibleMatch))
                    return otherString.substr(index, matchingLength);
                index++;
            }
            matchingLength--;
        }
        return '';
    };

    String.prototype.diff = function(newValue) {

        var largestMatch = this.largestMatch(newValue),
            preNew, postNew, preOld, postOld;

        if (!largestMatch) {
            return '<span class = "deleted">' + this + '</span><span class = "inserted">' + newValue + '</span>';
        } else {
            preNew = newValue.substr(0, newValue.indexOf(largestMatch));
            preOld = this.substr(0, this.indexOf(largestMatch));
            postNew = newValue.substr(preNew.length + largestMatch.length);
            postOld = this.substr(preOld.length + largestMatch.length);
            console.log({
                old: this.toString(),
                new: newValue,
                preOld: preOld,
                match: largestMatch,
                postOld: postOld,
                preNew: preNew,
                match2: largestMatch,
                postNew: postNew
            });
            return preOld.diff(preNew) + largestMatch + postOld.diff(postNew);
        }
    };

    function setResult(msg){
        $('.result-msg').text(msg).show();
    }
    function hideResult(msg){
        $('.result-msg').text('').hide();
    }
});
</script>
]]></content>
        </entry>
            <entry>
            <title type="html"><![CDATA[Everything you need to know about laravel required validations]]></title>
            <link href="https://techsemicolon.github.io/blog/2019/05/30/laravel-validation-required-if-with-unless-all-and-custom-if/"/>
            <updated>2019-05-30T00:00:00+00:00</updated>
            <id>https://techsemicolon.github.io/blog/2019/05/30/laravel-validation-required-if-with-unless-all-and-custom-if/</id>
            <content type="html"><![CDATA[<p>One of the strong part of the laravel core is the set of <code>validations</code> which validate your application's incoming data. One of the most common validation rule which is used in applications is <code>required</code>. We will see how can we make use of different types of required rules as well as build custom required rules conditionally.</p>

<ul>
<li>Required rule :</li>
</ul>

<p>As per the laravel's documentation. when we use <code>required</code> rule, the field under validation must be present and also it should be non-empty. Empty includes null value, empty string, empty array and uploaded file with no path.</p>

<p>In below request both <code>first_name</code> and <code>last_name</code> should be present and non-empty.</p>

<pre><code class="php">$request-&gt;validate([
    'first_name' =&gt; 'required|unique:posts|max:255',
    'last_name' =&gt; 'required',
]);
</code></pre>

<ul>
<li>Required rule based on other/supporting field(s) :</li>
</ul>

<p>Many times when designing complex and nested forms, we need to have required validation <code>only if</code> an other incoming data field is present. Now there are multiple cases in this :</p>

<ol>
<li>When you want <code>just single supporting field to be present</code>, then  you can use <code>required_with</code> like below :</li>
</ol>

<p>In below request the <code>city</code>, <code>state</code>, <code>zipcode</code> and <code>country</code> will be required if <code>address_line</code> is present in the incoming data.</p>

<pre><code class="php">$request-&gt;validate([
    'address_line' =&gt; 'sometimes|boolean',
    'city' =&gt; 'required_with:address_line',
    'state' =&gt; 'required_with:address_line',
    'zipcode' =&gt; 'required_with:address_line',
    'country' =&gt; 'required_with:address_line'
]);
</code></pre>

<ol start="2">
<li>When you want <code>any one of the supporting set of fields just to be present</code>, then  you can use <code>required_with</code> and pass multiple fields like below :</li>
</ol>

<p>In below request the we want to set validation rule for the <code>notify_new_updates</code> field. We can not notify if we do not have either of user phone number, email or fax. So the validation  <code>required_with:phone,email,fax</code> signifies that <code>notify_new_updates</code> will be required if one of the field phone, email or fax is present.</p>

<pre><code class="php">$request-&gt;validate([
    'name' =&gt; 'required|string',
    'phone' =&gt; 'sometimes|string',
    'email' =&gt; 'sometimes|email',
    'fax' =&gt; 'sometimes|string',
    'notify_new_updates' =&gt; 'boolean|required_with:phone,email,fax'
]);
</code></pre>

<ol start="3">
<li>When you want <code>all of the supporting set of fields to be present</code>, then you can use <code>required_with_all</code> and pass multiple fields like below :</li>
</ol>

<p>In below request the validation <code>required_with_all:phone,email,fax</code> signifies that <code>notify_new_updates</code> will be required if all the field phone, email and fax are present.</p>

<pre><code class="php">$request-&gt;validate([
    'name' =&gt; 'required|string',
    'phone' =&gt; 'sometimes|string',
    'email' =&gt; 'sometimes|email',
    'fax' =&gt; 'sometimes|string',
    'notify_new_updates' =&gt; 'boolean|required_with_all:phone,email,fax'
]);
</code></pre>

<ol start="4">
<li>When you want <code>any one of the supporting set of fields NOT to be present</code>, then  you can use <code>required_without</code> and pass one or more fields like below :</li>
</ol>

<p>In below request the <code>use_billing_as_shipping</code> will be required when <code>shipping_address</code> is NOT present.</p>

<pre><code class="php">$request-&gt;validate([
    'billing_address' =&gt; 'required|string',
    'shipping_address' =&gt; 'sometimes|string',
    'use_billing_as_shipping' =&gt; 'boolean|required_without:shipping_address'
]);
</code></pre>

<p>Similarly you can use <code>required_without_all</code> where it will need <code>sll of the supporting set of fields NOT to be present</code>.</p>

<ol start="5">
<li>When you want the other field not only to be present but also it should have a very specific value, then you can use <code>required_if</code> like below</li>
</ol>

<p>In below request the <code>admin_notification_email</code> will be required if and only if the <code>is_admin</code> field is present having value of <code>1</code>.</p>

<pre><code class="php">$request-&gt;validate([
    'is_admin' =&gt; 'required|boolean',
    'admin_notification_email' =&gt; 'required_if:is_admin,1|email',
]);
</code></pre>

<ol start="6">
<li>When you want the other field not only to be present but it should have a very specific value, then you can use <code>required_if</code> like below</li>
</ol>

<p>In below request the <code>displayname</code> will be required unless the <code>nickname</code> field is empty.</p>

<pre><code class="php">$request-&gt;validate([
    'nickname' =&gt; 'sometimes|string',
    'displayname' =&gt; 'required_unless:nickname,',
]);
</code></pre>

<ul>
<li>Custom and complex required if validation rules :</li>
</ul>

<p>We sometimes need to have complex set of conditional rule for required if. With laravel it's much easier than you could have imagined.</p>

<p>Laravel's <code>Rule</code> facade has <code>Rule::requiredIf()</code> method which we can use for this purpose. The beautify of this method is, it takes a boolean value or a closure which return a boolean value. This gives us lot of flexibility to have complex logic for required if rule.</p>

<ol>
<li>requiredIf() with simple boolean :</li>
</ol>

<p>Below example will make <code>employee_id</code> field required only if current user is an employee and the employee's company is still active.</p>

<pre><code class="php">$request-&gt;validate([
    'employee_id' =&gt; Rule::requiredIf($request-&gt;user()-&gt;is_employee &amp;&amp; $request-&gt;user()-&gt;employee-&gt;company-&gt;is_active)
]);
</code></pre>

<ol start="2">
<li>requiredIf() with closure :</li>
</ol>

<p>Below example will make <code>golden_discount_voucher</code> field required only if current user is a customer and has orders to fillfull criteria that the customer belongs to golden account category.</p>

<pre><code class="php">$request-&gt;validate([
    'golden_discount_voucher' =&gt; Rule::requiredIf(function() use($request){

        return  $request-&gt;user()-&gt;is_customer &amp;&amp;
                $request-&gt;user()-&gt;customer-&gt;orders-&gt;where('orders.grand_total', '&gt;', '1000')-&gt;count() &gt; 10;
    })
]);
</code></pre>
]]></content>
        </entry>
            <entry>
            <title type="html"><![CDATA[Composer stuck at Something&#039;s changed looking at all rules again]]></title>
            <link href="https://techsemicolon.github.io/blog/2019/05/20/composer-stuck--at-something-changed-looking-at-all-rules-error/"/>
            <updated>2019-05-20T00:00:00+00:00</updated>
            <id>https://techsemicolon.github.io/blog/2019/05/20/composer-stuck--at-something-changed-looking-at-all-rules-error/</id>
            <content type="html"><![CDATA[<p>While upgrading one of my laravel application from version 5.3 to 5.4, I came across a weird situation which was coming from composer dependency management. The situation was that when running composer install it was getting stuck at  <code>Something's changed looking at all rules again(1)</code> and composer was showing counter of attempts in brackets which was increasing from 1 to 900+. It took arund 2+ hours composer update was stuck in this situation and attempts counter was increasing, even when the system where application was hosted had pretty good configurations.</p>

<ul>
<li>How do I get this information :</li>
</ul>

<p>When you run composer command, lets say <code>composer install</code>, it does internal steps it's supposed to do one by one. However, we just see the high level output in command promt and not the details of steps. Similarly, the error message I saw can not be seen directly if you just do <code>composer install</code>. You need to make the output more verbose by passing <code>-v</code> command line argument. You can pass upto 3 v's to get more verbosity</p>

<p>For e.g. :</p>

<pre><code class="bash">`composer install -vvv`
</code></pre>

<p>When I did this, I got lot of detailed steps whats composer is doing internally, one of these verbose messages was the error message <code>Something's changed looking at all rules again(1)</code></p>

<ul>
<li>Why composer gets stuck :</li>
</ul>

<p>Okay, to explain it's comparatively easy to understand why it gets stuck. Let's take an example that application has 2 packages <code>techsemicolon/package-first</code> and  <code>techsemicolon/package-second</code>:</p>

<pre><code class="json">"require": {
    "php": "&gt;=5.6.4",
    "techsemicolon/package-first": "5.4.*",
    "techsemicolon/package-second": "2.3.*",
    .
    .
    .
    .
}
</code></pre>

<p>Now the both these packages have package <code>techsemicolon/dependency-package</code> in their own dependencies. But</p>

<pre><code>`techsemicolon/package-first` requires `techsemicolon/dependency-package` at version `2.*.*` 
`techsemicolon/package-second` requires `techsemicolon/dependency-package` at version `3.*.*`. 
</code></pre>

<p>Now, it's really confusing for composer to now decide if it should install the <code>techsemicolon/dependency-package</code> at version <code>2.*.*</code> or <code>3.*.*</code>. When we do composer install or update, it calculates these dependency trees and decide which version to install/update to. It checks the dependency rules defined in <code>composer.json</code> of application as well as those in individual package as their dependencies. Hence it gets stuck at <code>Something's changed looking at all rules again</code>.</p>

<p>Now, there can be situation where it recalculatues the rules again and shows this message but within a minute or two it finds the version which can satisfy the dependencies and proceeds. But in cases like mine, it calculates the rules in circles and gets stuck for really really long time.</p>

<ul>
<li>The solution :</li>
</ul>

<p>A note to start with, this is not the only or best solution to resolve this. But, considering this error is little different than commonly occuring composer errors, this is what I could do to resolve this quickly and effectively.</p>

<ol>
<li><p>Let's consider your composer has 15 packages in <code>require</code> dependency section of your app's main <code>composer.json</code>. I followed an approach which is very neive, I divided those 15 depencies into 3 sections of 5. I commented out first 5 and ran composer install. It if still gets stuck at error, then I commented out next 5 and uncommented first 5 and ran composer install again. I kept doing this until I found a faulty batch which if I comment out then composer install works properly. I kept composer install with verbosity of <code>--vvv</code>so it shows detailed output.</p>

<p>Then you will have n number of packages from the commented out batch, 5 in my case.</p></li>
<li><p>Then I went to <a href="https://packagist.org"><code>https://packagist.org</code></a> and went to each of that package from the faulty batch. I checked their internal dependencies or requirements of php version etc. Cross checked with other packages in the same batch if there is a conflict of version.</p></li>
<li><p>But I still did not find the dependency conflict. So I came back to my app where composer install worked when I commented out the failty batch. Then I did <code>composer show --tree</code>. That time I found that :</p></li>
</ol>

<pre><code>`barryvdh/laravel-dompdf` required `dompdf/dompdf` at version `^0.8` 
`laravel-datatables-oracle` required `dompdf/dompdf` at version `^0.7`
</code></pre>

<p>I had to upgrade version of <code>laravel-datatables-oracle</code> so that its dependency of <code>dompdf/dompdf</code> is at version <code>^0.8</code> which matches with <code>barryvdh/laravel-dompdf</code>'s installed version.</p>

<p>Don't worry, once you do this in your own app you will figure it out that it's not that hard. I freaked out myself when I saw the error for the first time.</p>

<ul>
<li>Quick note on composer diagnosis :</li>
</ul>

<p>You can run <code>composer diagnose</code> which gives you suggestions and warnings to improve the <code>composer.json</code> package version calculations. e.g.</p>

<pre><code>require.barryvdh/laravel-debugbar : exact version constraints (2.3.2) should be avoided if the package follows semantic versioning
require.webmozart/assert : exact version constraints (1.2) should be avoided if the package follows semantic versioning
</code></pre>
]]></content>
        </entry>
            <entry>
            <title type="html"><![CDATA[Laravel content-length header issue with gzip compression]]></title>
            <link href="https://techsemicolon.github.io/blog/2019/04/23/laravel-content-length-issue-with-gzip-nginx-compression/"/>
            <updated>2019-04-23T00:00:00+00:00</updated>
            <id>https://techsemicolon.github.io/blog/2019/04/23/laravel-content-length-issue-with-gzip-nginx-compression/</id>
            <content type="html"><![CDATA[<p>As an optimization process from server side responses, we use gzip compression which compresses the response before sending it to the client. Compressing responses as a result significantly reduces the size of data being transmitted.</p>

<p>As you can imagine any type of compression requires certain amount of memory and processing power. Considering the gzip compression happens at runtime, it consumes considerable processing resources. Which is why it is important to configure the compression for the responses which really need it.</p>

<ul>
<li>Configuring compression :</li>
</ul>

<p>An application web server has different types of responses, some might be extremely small having size of 10byte or some with very large data having size more than 1MB. It is fairly good to consider  that the 10byte response does not need compression as it's already very very tiny in size. If your server is attempting to compresses such smal responses as well, its consuming unnccessary resources in response compression.</p>

<p>To avoid this, we use <code>gzip directives</code> to notify gzip compression mechanism that only compress the responses which have size greater then the defined directive value. We use <code>gzip_min_length</code> directive for it which takes numeric value representing <code>bytes</code>.</p>

<pre><code>gzip_min_length 1000;
</code></pre>

<p>As per above gzip configuration, any response having size less than 1000 bytes will be transmitted to the client as it is without any compression. Any response having size greater than 1000 bytes will be compressed first at runtime and then transmitted to the client.</p>

<p>Note : The value of <code>1000</code> bytes is taken just as an example for this article. Please change it as per your requirements.</p>

<ul>
<li>Now here is the catch :</li>
</ul>

<p>Even if you have the <code>gzip_min_length</code> directive value set, it under the hood relies on <code>Content-Length</code> header present in response.</p>

<p><code>The gzip module during the compression determines the length only from the “Content-Length” response header field</code></p>

<p>So, if your app server is <code>NOT</code> having <code>Content-Length</code> header, <code>gzip_min_length</code> directive is not going to determine the response size and hence it will default to <code>all responses being compressed</code> before their transmittion to client.</p>

<p>This is pretty bad for a server which is serving lot of requests as nginx    gzip is utilizing unneccessary resources for compression of responses very tiny in size.</p>

<ul>
<li>The solution :</li>
</ul>

<p>The solution is fairly simple, we need to add <code>Content-Length</code> in each of our response so that gzip can determine the response size and decide if compression is to be done or not.</p>

<p>As we are focusing on laravel in this article, we can add content length using a simple middleware. Let's call it <code>ContentLegthMiddleware</code> :</p>

<pre><code class="php">&lt;?php

namespace App\Http\Middleware;

use Closure;

class ContentLegthMiddleware
{
    /**
     * Handle an incoming request.
     *
     * @param  \Illuminate\Http\Request  $request
     * @param  \Closure  $next
     * @return mixed
     */
    public function handle($request, Closure $next)
    {
        $response = $next($request);

        // Get response length
        $responseLength = strlen($response-&gt;getOriginalContent());

        // Add the header
        $response-&gt;header('Content-Length', $responseLength);

        // Return the response
        return $response;
    }
}
</code></pre>

<p>Last but not the least, you need to add the middleware in <code>app/Http/Kernel.php</code>.</p>

<p>Now you will be able to see <code>Content-Length: xxx</code> in your dev tools under response headers.</p>

<ul>
<li>Benchmarking :</li>
</ul>

<p>It is important to see if these changes have made any difference. I used <a href="https://github.com/JoeDog/siege"><code>Siege</code></a> load testing framework to test my server before and after the above changes were done. I could definitely see quicker response times and less CPU utilization. Please note that, this will be significantly seen on a high workload. If you have couple of requests on your server, the difference in performance will be negligible.</p>

<ul>
<li>Quick note on gzip_comp_level :</li>
</ul>

<p>The gzip compression as another directive called <code>gzip_comp_level</code> which receives values from <code>1</code> to <code>9</code>, where compression increases from 1 to 9. This is another vital directive. If you have comression level, it is going to consume more resources for compressing the response. For most servers value of 2 or 3 is enough as difference between compression from level to level is not significant.</p>
]]></content>
        </entry>
    </feed>