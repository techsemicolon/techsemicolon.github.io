<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title><![CDATA[Tech.Semicolon]]></title>
    <link href="https://techsemicolon.github.io/blog/tags/fpm.xml" rel="self"/>
    <link href="https://techsemicolon.github.io/"/>
    <updated>2020-02-05T12:38:41+00:00</updated>
    <id>https://techsemicolon.github.io/</id>
        <generator uri="http://sculpin.io/">Sculpin</generator>
            <entry>
            <title type="html"><![CDATA[Stream Nginx and PHP FPM logs into AWS cloudwatch]]></title>
            <link href="https://techsemicolon.github.io/blog/2020/02/04/stream-nginx-fpm-logs-into-aws-clodwatch/"/>
            <updated>2020-02-04T00:00:00+00:00</updated>
            <id>https://techsemicolon.github.io/blog/2020/02/04/stream-nginx-fpm-logs-into-aws-clodwatch/</id>
            <content type="html"><![CDATA[<p>When you have a web application running on AWS instances, you may have control over some cloud monitors of the instances such as <code>Memory Usage</code>, <code>CPU Utilization</code> or even metrics related to storage space. But :</p>

<ul>
<li>What if you want to have your nginx and PHP FPM logs steamed on AWS?</li>
<li>You want to set up a cloudwatch alarm if nginx throws an error?</li>
<li>You have an autoscaling setup and you want to steam nginx logs of all servers at a central log group on cloudwatch?</li>
<li>You do not want to incorporate any 3rd party monitoring tool and want to stick to AWS?</li>
<li>Last but not the least, you want it to be cost effective as logs are going to pile up as time goes?</li>
</ul>

<p>Well, this is all possible now! Let's find out how...</p>

<h4 id="aws-cloudwatch-agent-%3A">AWS Cloudwatch Agent :</h4>

<p>The first and very basic question comes to mind when streaming any custom log file into AWS cloudwatch is : <code>How I am going to stream the logs continously to AWS cloudwatch</code> The answer is using <a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Install-CloudWatch-Agent.html"><code>AWS Cloudwatch Agent</code></a></p>

<p>On a broader level, once you install AWS Cloudwatch Agent on your EC2 instance, it's going to have a service called <code>awslogs</code> available for you. This service is responsible for streaming any log file you wish to track on AWS cloudwatch. This service will take care of how to push the logs, how much log buffer to push at a time, how much kbs of data to push at a time and so on.</p>

<p>As any unix service would have, the <code>awslogs</code> service also has very handy configuration file using which you can tweak the above parameters which contribute to pushing the log stream into AWS cloudwatch.</p>

<h3 id="giving-permissions-to-push-to-cloudwatch-%3A">Giving permissions to push to cloudwatch :</h3>

<p>When we have the AWS Cloudwatch agent installed and the <code>awslogs</code> service running, you will expect the log streaming on AWS cloudwatch inside AWS region you specified. But, it will not work directly.</p>

<p>This is because, the instance should have permission to push logs into cloudwatch. For this we will create a new policy called <code>AWS-cloudwatch-agent-policy</code>.</p>

<ol>
<li>Login to AWS Console and go to <code>IAM</code> service.</li>
<li>Go to <code>Policies</code> from the L.H.S. menu</li>
<li>Click on <code>Create Policy</code></li>
<li>A visual editor for policy will be shown. But we are going to save time here. Click on the <code>json</code> tab and enter following json in there :</li>
</ol>

<pre><code class="json">{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "logs:CreateLogGroup",
                "logs:CreateLogStream",
                "logs:PutLogEvents",
                "logs:DescribeLogStreams"
            ],
            "Resource": [
                "arn:aws:logs:*:*:*"
            ],
            "Condition": {
                "StringEquals": {
                    "aws:RequestedRegion": "xx-xxxx-x"
                }
            }
        }
    ]
}
</code></pre>

<p>Make sure you replace <code>xx-xxxx-x</code> with the region you want to steam logs into. e.g. <code>us-west-1</code>
5. Click on <code>Review Policy</code>
6. Give <code>Name</code> as <code>AWS-cloudwatch-agent-policy</code>. If you want to be specific(which I would recommend), add the region name in there e.g. <code>AWS-cloudwatch-agent-policy-us-west-1</code>
7. Give <code>Description</code> and click on <code>Create Policy</code></p>

<p>Now, we have the policy ready. We need to attach this policy to an <code>IAM User</code> or <code>IAM Role</code> so that EC2 instance can stream logs into AWS cloudwatch. There are 2 ways to do it :</p>

<p>A. If your EC2 instance is having an instance IAM role attached to it, then simply attach this policy to that role. You can find this by going into <code>EC2 instances</code> section and looking for instance property called <code>IAM Role</code>.</p>

<p>OR</p>

<p>B. If you do not have any IAM role attached to your instance, then create a new IAM user with name <code>AWS-cloudwatch-agent-user</code> with ONLY <code>programatic access</code> enabled. Then attach the policy we created above to this user. Make sure you save the <code>Access Keys</code> and <code>Secret Access Keys</code> for this user. We will need it shortly.</p>

<h4 id="installing-the-agent-%3A">Installing the agent :</h4>

<p>Now that we have the permissions sorted out, let's install AWS cloudwatch agent, which is the easiest task to do. I am using an <code>ubuntu</code> EC2 instance so I will follow the steps relevent to it in this article. If you are using any other operating systems, feel free to visit the <a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/install-CloudWatch-Agent-on-EC2-Instance.html">installation documentation</a></p>

<ol>
<li>SSH into your ubuntu EC2 instance</li>
<li>Run following commands :</li>
</ol>

<pre><code class="bash"># Switch to root user
sudo -s
# Update the packages
apt-get update -y
# Download the ubuntu cloudwatch agent setup file
cd /root
curl https://s3.amazonaws.com/aws-cloudwatch/downloads/latest/awslogs-agent-setup.py -O
# Install the cloudwatch agent
# Make sure you replace xx-xxxx-x with your region e.g. us-west-1
sudo python ./awslogs-agent-setup.py --region=xx-xxxx-x
</code></pre>

<ol start="3">
<li>Once you run the <code>awslogs-agent-setup.py</code>, it will ask you couple of questions in the prompt like below :</li>
</ol>

<p>When it will ask for <code>AWS Access Key ID</code> &amp; <code>AWS Secret Access Key</code>, if you have chosen the option A in previous section which is using <code>IAM Role</code> of the instance, then press continue(enter) without anything. If you have chosen the option B which is using an <code>IAM User</code> with programatic access, add the <code>AWS Access Key ID</code> &amp; <code>AWS Secret Access Key</code> of that user and then proceed with the prompt.</p>

<p>On the remaining prompt, just follow what we have below. We will just start with basic bare bones because, we are going to update these settings from config file in next step.</p>

<pre><code class="bash">Step 1 of 5: Installing pip ...libyaml-dev does not exist in system DONE

Step 2 of 5: Downloading the latest CloudWatch Logs agent bits ... DONE

Step 3 of 5: Configuring AWS CLI ...
AWS Access Key ID [None]:
AWS Secret Access Key [None]:
Default region name [us-west-1]:
Default output format [None]:

Step 4 of 5: Configuring the CloudWatch Logs Agent ...
Path of log file to upload [/var/log/syslog]:
Destination Log Group name [/var/log/syslog]:

Choose Log Stream name:
  1. Use EC2 instance id.
  2. Use hostname.
  3. Custom.
Enter choice [1]: 1

Choose Log Event timestamp format:
  1. %b %d %H:%M:%S    (Dec 31 23:59:59)
  2. %d/%b/%Y:%H:%M:%S (10/Oct/2000:13:55:36)
  3. %Y-%m-%d %H:%M:%S (2008-09-08 11:52:54)
  4. Custom
Enter choice [1]: 3

Choose initial position of upload:
  1. From start of file.
  2. From end of file.
Enter choice [1]: 2
More log files to configure? [Y]: N

Step 5 of 5: Setting up agent as a daemon ...DONE


------------------------------------------------------
- Configuration file successfully saved at: /var/awslogs/etc/awslogs.conf
- You can begin accessing new log events after a few moments at https://console.aws.amazon.com/cloudwatch/home?region=us-west-1#logs:
- You can use 'sudo service awslogs start|stop|status|restart' to control the daemon.
- To see diagnostic information for the CloudWatch Logs Agent, see /var/log/awslogs.log
- You can rerun interactive setup using 'sudo python ./awslogs-agent-setup.py --region us-west-1 --only-generate-config'
</code></pre>

<h3 id="testing-the-initial-setup-%3A">Testing the initial setup :</h3>

<p>Let's first check if the <code>awslogs</code> service is running :</p>

<pre><code class="bash">sudo service awslogs status
</code></pre>

<p>This should show as active.</p>

<p>Now let's see if our instance is streaming anything :</p>

<pre><code class="bash">sudo tail -f /var/log/awslogs.log
</code></pre>

<p>You should either see something like <code>cwlogs.push.publisher</code> which is publishing log successfully. If you have any issue with permissions, you should see the error here.</p>

<p>If everything is working, you should see a log group called <code>/var/log/syslog</code> when you visit https://console.aws.amazon.com/cloudwatch/home?region=xx-xxxx-x#logs: where <code>xx-xxxx-x</code> is your AWS region. If you click on the log group, you should see syslogs streaming in there. whohooo!</p>

<p><img src="/images/aws-cloudwatch-agent/log-group.png" alt="AWS CloudWatch Log Group" /></p>

<p><img src="/images/aws-cloudwatch-agent/log-stream.png" alt="AWS CloudWatch Log Stream" /></p>

<h3 id="updating-the-configurations-%3A">Updating the configurations :</h3>

<p>Let's first stop the <code>awslogs</code> service for a moment to avoid any syslogs steaming into AWS cloudwatch.</p>

<pre><code class="bash">sudo service awslogs stop
</code></pre>

<p>Now, we are going to update the configuration file <code>/var/awslogs/etc/awslogs.conf</code>. Edit this file and add below contents in it :</p>

<pre><code>[general]
state_file = /var/awslogs/state/agent-state

[nginx]
file = /var/log/nginx/error.log
log_group_name = nginx_error_logs
log_stream_name = {instance_id}-{hostname}
datetime_format = %b %d %H:%M:%S
initial_position = end_of_file

[phpfpm]
file = /var/log/php7.2-fpm.log
log_group_name = php_fpm_logs
log_stream_name = {instance_id}-{hostname}
datetime_format = %b %d %H:%M:%S
initial_position = end_of_file
</code></pre>

<p>Note : Make sure you verify the log file paths as per your server settings.</p>

<p>Let's quickly go over what we have set. The <code>general</code> section is what this service needs to keep its state intact. We have kept it as it is.</p>

<p>The next 2 sections <code>nginx</code> and <code>phpfpm</code> will stream the logs. 
1. <code>file</code> : The absolute path of the respective log file
2. <code>log_group_name</code> : Log group which will cloud all similar logs together in AWS cloudwatch
3. <code>log_stream_name</code> : The name of the stream of this log group pushed from an instance
4. <code>datetime_format</code> : The format of logged timestemp
5. <code>initial_position</code> : If you would like to stream the new lines from end of the time or from begining of the file(start_of_file)</p>

<p>If you wish to know more about these options, visit this <a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/AgentReference.html">documentation</a></p>

<p>Once you save this file, start the <code>awslogs</code> service :</p>

<pre><code class="bash">sudo service awslogs start
</code></pre>

<p>Finally, you will now see nginx and phpfpm logs steaming into your AWS cloudwtch logs in their respective log groups.</p>

<h4 id="avoiding-streaming-of-notices-and-warnings-%3A">Avoiding streaming of notices and warnings :</h4>

<p>At this point in your setup, the <code>awslogs</code> service will stream every log entry from the files you are streaming, no matter if it's an <code>info</code>, <code>notice</code>, <code>warning</code> or an <code>error</code>.</p>

<p>You would not want to pile up your AWS cloudwatch logs with anything which you do not want to track. You can get around this from choosing either of below options :</p>

<p>A. Updating nginx and php fpm service configurations and making sure their log levels are set just to log <code>errors</code> or <code>warnings</code> if you want.
B. You can use the <code>awslogs</code> configuration option called <code>multi_line_start_pattern</code> where you can specify a regex which will determine the start of the line.</p>

<h4 id="setting-up-a-cloudwatch-alarm-%3A">Setting up a cloudwatch alarm :</h4>

<p>Now that we have the nginx and phpfpm logs into AWS cloudwatch, it's time to do the main step. Let's set up an alarm.</p>

<ol>
<li>Login to AWS Console and go to <code>Cloudwatch</code> service.</li>
<li>Go to <code>Alarms &gt; Alarm</code> from the L.H.S. menu</li>
<li>Click on <code>Create Alarm</code></li>
<li>Click on <code>Select Metric</code> and search for <code>Log Group</code>, you will see <code>Logs &gt; Log Group Metrics</code>, select that</li>
<li>From <code>LogGroupName</code>, select the log group you want to monitor. You should see <code>nginx_error_logs</code> and <code>php_fpm_logs</code> in the listing, choose any one of them</li>
<li>You will see the screen where we specify the alarm conditions. You can set duration of 5 minutes for a static threshold when incoming logs are greater than 50. This means that if for a period of 5 minutes, if any of the log group we select have more than 50 logs, this alarm will go on.</li>
</ol>

<p><img src="/images/aws-cloudwatch-agent/log-alarm.png" alt="AWS CloudWatch Log Alarm" /></p>

<h4 id="pricing-%3A">Pricing :</h4>

<p>The AWS cloudwatch agent is free because it's a native service running on your instance. However, what you pay is for the amount of logs you collect, stream and store over AWS cloudwatch. You can check the pricing from their <a href="https://aws.amazon.com/cloudwatch/pricing/">documentation</a>. However, you will find it's much cheaper than most of the 3rd party services providing similar log monitoring service.</p>

<h4 id="final-words-%3A">Final words :</h4>

<p>It's said that you can spread over different cloud providers or service providers to explore more. But in my experience, AWS has these native options which are cost effective, easy and standard to set up.. Plus you have everything in one console, you should definitely give it a try before shifting to a 3rd party solution implementing the same thing.</p>
]]></content>
        </entry>
            <entry>
            <title type="html"><![CDATA[PHP OPcache important settings and revalidation simplified]]></title>
            <link href="https://techsemicolon.github.io/blog/2019/10/05/php-fpm-and-opcache-settings-and-invalidation-simplified/"/>
            <updated>2019-10-05T00:00:00+00:00</updated>
            <id>https://techsemicolon.github.io/blog/2019/10/05/php-fpm-and-opcache-settings-and-invalidation-simplified/</id>
            <content type="html"><![CDATA[<h3 id="about-opcache-%3A">About OPcache :</h3>

<p>PHP as a language is an interpreted. But the interpretation happens from the binaries. These binaries are compiled into intermediate bytecodes. OPcache comes into picture during the compilation phase where it stores the precompiled script's bytecodes into a shared memory. When next time PHP needs to compile the same PHP script, the precompile bytecodes from the shared memory are used.. Therefore removing the requirement for PHP to load and parse scripts on each request. As a result improving overall PHP performance.</p>

<p>The OPcache extension is bundled with PHP 5.5.0 and later versions.</p>

<h3 id="why-it-is-needed-%3A">Why it is needed :</h3>

<p>Okay wait... Let's really get into a very vital need of this OPcache in the ever growing world of PHP frameworks. Any PHP framework has some core package files which we never change. These php package files really let us use the framework capabilities. For example for laravel, we have <code>illuminate</code>'s laraval packages. Now on top of that, we install new PHP packages as and when needed using dependency managers like <code>composer</code>. We hardly change these files unless we update/install/remove a package dependency.</p>

<p>Now, independent of which PHP framework you are using, these core PHP script files which constitute framework more or less are compiled for each request to generate their bytecodes for interpretation. Similarly, the dependency package PHP script are compied as per their need for that particular request.</p>

<p>Why do we really need to compile these core PHP script files which we indeed hardly change?? Isn't that an overly unneccessary overhead for PHP compilation process to compile these hundreds of PHP script files on each request and thenm intrepret them? Doesn't make much sense right? There you go.. OPcache to the rescue...</p>

<h3 id="important-settings-of-opcache-%3A">Important settings of OPcache :</h3>

<p>As OPcache overall sounds really fascinating which internally optimizes the compilation process of PHP scripts, it is equally important to know the important settings. Cache handling, cache clearing and cache storing and overall invalidation depends on these settings.</p>

<ul>
<li>Finding the php.ini to work with : 
You need to find the php.ini which is currently in use. There are different ways to find it. You can find it from terminal using :</li>
</ul>

<pre><code class="bash">php --ini | grep -i loaded
</code></pre>

<p>OR create a temporary script file called 'info.php` and have following content on it :</p>

<pre><code class="php">phpinfo();
</code></pre>

<p>Once you open this file into browser, you would be able to see which php.ini is used.</p>

<p>Mostly if you have php version 7.x with fpm installed then it will be inside <code>/etc/php/7.x/fpm/php.ini</code>. Or if you have 7.x with apache2 installed then it will be inside <code>/etc/php/7.x/apache2/php.ini</code>.</p>

<ul>
<li><p>Enable opcache :</p>

<p>In the loaded and used <code>php.ini</code>, you can enable and disable opcache by updating following directive :</p>

<pre><code>opcache.enable=1 //Enables OPcache
opcache.enable=0 //Disables OPcache
</code></pre>

<p>OPCache also can be enabled for CLI, PHP running from the command line interface(terminal) :</p>

<pre><code>opcache.enable_cli=1 //Enables OPcache
opcache.enable_cli=0 //Disables OPcache
</code></pre></li>
<li><p>OPcache RAM usage :</p>

<p>When the entire cache is stored into shared memory, the default max size of that memory storage is <code>64MB</code> before PHP <code>7.0.0</code> and <code>128MB</code> for after versions. You can update it as per your memory. Ideally 64MB to 128MB size is more than enough for most of the PHP frameworks.
In the loaded and used <code>php.ini</code>, you can change this setting by updating following directive :</p>

<pre><code>opcache.memory_consumption=128
</code></pre></li>
<li><p>Cached script limit :</p>

<p>By default OPcache can cache <code>2000</code> files before PHP <code>7.0.0</code> and <code>10000</code> for after versions. You can increase this number as per your requirement. For frameworks like PHP Magento you could probably have this value bumped up as the core files are large in number.</p>

<p>First know count of PHP scripting files in your codebase :</p>

<pre><code class="bash"># Make sure you change this path as per your setup
cd  /var/www/html 
# Count number of php files
find . -type f -print | grep php | wc -l
</code></pre>

<p>Now above will givbe you a numeric count of total PHP script files in your codebase. Now you can set OPcache setting accordingly. In the loaded and used <code>php.ini</code>, you can change this setting by updating following directive :</p>

<pre><code>opcache.max_accelerated_files=20000
</code></pre></li>
<li><p>Set automated cache clearing :</p>

<p>When you have new updates in cached PHP script files, OPcache will attempt to clear and then revalidate their cache. This is set using <code>validate_timestamps</code> directive and it happens periodically based on other directive setting <code>revalidate_freq</code>. When this directive is disabled, you MUST reset OPcache manually via opcache_reset(), opcache_invalidate() or by restarting the Web server for changes to the filesystem to take effect.
In the loaded and used <code>php.ini</code>, you can change this setting by updating following directive :</p>

<pre><code>opcache.validate_timestamps=1 //Enabled
opcache.validate_timestamps=0 //Disabled
</code></pre></li>
<li><p>Revalidation and it's frequency :</p>

<p>OPcache revalidates the opcache afer certain number of seconds to check if your code has changed. <code>0</code> value signifies that it checks your PHP code every single request which is unneccessary.
By default it happens every 2 seconds. If you have PHP files changing hardly on your setup, it makes sence to increase the frequency duration. As revalidation also takes memory, bumping this up might save unneccessary memory revalidation usage.
In the loaded and used <code>php.ini</code>, you can change this setting by updating following directive :</p>

<pre><code>opcache.revalidate_freq=120
</code></pre>

<p>OR you can totally disable this revalidation by updating following directive :</p>

<pre><code>opcache.validate_timestamps=0 //Disables OPcache revalidation
</code></pre>

<p>In your development or sandbox environment you can set this to 0 so that changes take effect immediately. However, it is as good as disabling which can save lot of syscalls.</p></li>
<li><p>Skipping caching selecively :</p>

<p>If you know that let's say <code>user.php</code> file in your framework changes often and you do not really want to clear cache everytime it changes. You can basically have OPcache ignore the caching of this file. It is called blacklisting file in terms of OPcache. This setting is littlbit different. It takes an absolute path location of a <code>.txt</code> file. In this text file you can specify the files you want to blacklist(skip) for caching.
In the loaded and used <code>php.ini</code>, you can change this setting by updating following directive :</p>

<pre><code>opcache.blacklist_filename=/etc/php/7.x/fpm/opcache_blacklist.txt
</code></pre>

<p>You can have whatever name you want for this text file. And then inside <code>opcache_blacklist.txt</code> :</p>

<pre><code class="bash">#Specific file inside /var/www/html/
/var/www/html/user.php 

#All files starting with user* inside /var/www/html/
/var/www/html/user

#Wildcare usage inside /var/www/html/
/var/www/*-user.php
</code></pre></li>
<li><p>Increasing request response shutdown :</p>

<p>PHP OPcache uses a mechanism which checks for repetitive occurances of strings and internally stores single value of it with remaining ones having pointer to the same value. The fascinating thing is instead of having a pool of these strings for each and every FPM process, this shares it across all of your FPM processes.</p>

<p>In OPcache settings, you can control how much memory this process can use from directive <code>opcache.interned_strings_buffer</code> Its default value is <code>4</code>(MB) before PHP <code>7.0.0</code> and <code>8</code>(MB) for after versions.</p>

<p>In the loaded and used <code>php.ini</code>, you can change this setting by updating following directive :</p>

<pre><code>opcache.interned_strings_buffer=12
</code></pre></li>
</ul>

<p>Note that to have changes to take effect from php.ini, you need to restart PHP FPM or apache2 depending on your setup.</p>

<h3 id="clearing-the-opcache-%3A">Clearing the OPcache :</h3>

<p>When OPcache is enabled, any changes in cached PHP script files will not take effect until OPcache is cleared or it is revalidated. This is applicable when you release new updates into a OPcache enabled PHP server.</p>

<p>To clear cache there are multiple ways :</p>

<ul>
<li><p>Clearing from browser :</p>

<p>You can have a file <code>opcache_refresh.php</code> with following content :</p>

<pre><code class="php">try{
    opcache_reset();
    echo "OPcache has been cleared successfully!";
}
catch(\Exception $e){
    echo "Oops.. OPcache could not be cleared!";
}
</code></pre>

<p>When you request this file from browser, it will reset(clear) the opcache and any new changes done inside the cached files will start reflecting.</p></li>
<li><p>Clearing from terminal :</p>

<p>If you have nginx with PHP FPM :</p>

<pre><code class="bash">sudo service php7.x-fpm reload
</code></pre>

<p>If you have apache2 :</p>

<pre><code class="bash">sudo service apache2 reload
</code></pre></li>
<li><p>Using cachetool :</p>

<p>This is very useful when you do not want to have an open PHP file to reset opcache and you do not want to reload FPM or apache2 for the same. <a href="http://gordalina.github.io/cachetool/"><code>cachetool</code></a> is a way between the two :</p>

<p>To Install  :</p>

<pre><code class="bash">// Download the phar
curl -sO http://gordalina.github.io/cachetool/downloads/cachetool.phar
// Set permissions
chmod +x cachetool.phar
</code></pre>

<p>Now you can clear opcache using following command :</p>

<pre><code class="bash">// Using an automatically guessed fastcgi server
php cachetool.phar opcache:reset --fcgi

// Using php fpm socket
php cachetool.phar opcache:reset --fcgi=/var/run/php7.x-fpm.sock
</code></pre></li>
</ul>

<h3 id="opcache-and-the-automated-deployments-%3A">OPcache and the automated deployments :</h3>

<p>When you have automated deployments set up on your PHP server, you need to incorporate OPcache clearing so that new changes take effect.</p>

<p>If you do not perform this manually, eventually OPcache will revalidate itself based on your OPcache revalidation settings assuming you have <code>validate_timestamps</code> set to <code>1</code>. But it is pretty handy to do it manually as part of your automated deployment scripts to make sure changes take immediate effect.</p>

<p>From my personal experience, relading FPM or apache2 to clear OPcache is not a good option. Even if it gracefully reloads PHP processes, any request which is(unfortunarely) ongoing on the server gives <code>502 bad gateway</code>. I would rather use the alternative like <code>cachetool</code> mentioned in above section which does not affect any ongoing PHP processes at all.</p>

<p>The sequence when you reload OPcache matters. As soon as git or any of your deployment methods pulls the latest PHP code changes, you should first clear the OPcache. Then you can run all the remaining deployment commands which may use the updated PHP files. This makes sure the deployment commands do not use cached PHP script opcodes even when the sever has pulled the latest changes.</p>
]]></content>
        </entry>
    </feed>